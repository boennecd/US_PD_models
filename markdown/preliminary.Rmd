---
title: "Model without random effects"
author: 
- Rastin Matin
- Benjamin Christoffersen 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:  
  html_document:
    toc: true
bibliography: refs.bib
nocite: | 
  @Chava04, @Lando13
---

<script>
$(document).ready(function(){
  var hide_divs = $("div.hideable");
  hide_divs.each(function(){
    // Wrap content in div
    $(this).wrapInner( "<div class='hideable_content', style='display: none;'></div>");
    
    // Add button
    $(this).prepend("<button id='toogle'>show</button>");
  });
  
  // Add hideable btn
  // Put the rest in a div
  
  $("div.hideable button#toogle").click(function(){
    var parent = $(this).parent();
    var target_div = $(parent).find("div.hideable_content");
    
    if(target_div.css("display") == "none"){
      target_div.show();
        $(this).text("Hide");
    } else {
      target_div.hide();
      $(this).text("Show");
    }
  });
});
</script>

## Load data

```{r static_setup, include=FALSE, cache=FALSE}
# please do not set options here that could change...
knitr::opts_chunk$set(
  cache.path = 
    paste0(file.path("cache", "preliminary"), .Platform$file.sep), 
  fig.path = 
    paste0(file.path("fig"  , "preliminary"), .Platform$file.sep))
```

```{r setup, include=FALSE, cache=FALSE}
# please do set options here that could change...
knitr::opts_chunk$set(
  echo = TRUE, fig.height = 4, fig.width = 7, dpi = 72, comment = "#R", 
  error = FALSE)
options(digits = 4, scipen = 10, width = 90)
```

```{r run_setup, child = 'setup.Rmd'}
```

Fit models with only linear association on the linear predictor scale

<div class="hideable">

```{r check_rebuild_extra, include = FALSE}
if(!interactive()){
  .check_before_merge <- file.path("markdown", "cache", "preliminary_check")
  if(!file.exists(.check_before_merge)){
    knitr::opts_chunk$set(cache.rebuild = TRUE)
  } else
    knitr::opts_chunk$set(
      cache.rebuild = knitr::opts_chunk$get("cache.rebuild") ||
        !readRDS(.check_before_merge) == digest::digest(dat))
  
  saveRDS(digest::digest(dat), .check_before_merge)
}
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "winz", 
      path = paste0(file.path("markdown", "cache", "preliminary"), 
                    .Platform$file.sep))
-->

```{r winz, cache = 1}
######
# fit winsorized model
f1 <- glm(
  y ~ wz(r_wcapq_atq) + wz(r_req_atq) + 
  wz(r_oiadpq_atq) + wz(r_mv_ltq) + wz(r_saleq_atq) + wz(r_niq_atq) + 
  wz(r_ltq_atq) + wz(r_actq_lctq) + wz(sigma) + 
  wz(excess_ret) + wz(rel_size), binomial("cloglog"), dat)
summary(f1)
AIC(f1)

#####
# what if we winsorize at another level?
pr <- c(.025, .975)
f2 <- glm(y ~  wz(r_wcapq_atq, pr) + wz(r_req_atq, pr) + 
  wz(r_oiadpq_atq, pr) + wz(r_mv_ltq, pr) + wz(r_saleq_atq, pr) + 
  wz(r_niq_atq, pr) + wz(r_ltq_atq, pr) + wz(r_actq_lctq, pr) + 
  wz(sigma, pr) + wz(excess_ret, pr) + wz(rel_size, pr), 
  binomial("cloglog"), dat)
summary(f2)
AIC(f1, f2)

#####
# what if we use another numerator instead of total assets? As of this 
# writing, this is 50% of the market value and 50% of the total assets
f3 <- glm(
  y ~ wz(r_wcapq_nn) + wz(r_req_nn) + 
  wz(r_oiadpq_nn) + wz(r_mv_ltq) + wz(r_saleq_nn) + wz(r_niq_nn) + 
  wz(r_ltq_nn) + wz(r_actq_lctq) + wz(sigma) + 
  wz(excess_ret) + wz(rel_size), binomial("cloglog"), dat)
summary(f3)
AIC(f1, f2, f3)

#####
# what if we add distance-to-default to this model
f4 <- update(f1, . ~ . + wz(dtd))
summary(f4)
AIC(f1, f2, f3, f4)

#####
# what if we add macro variables
f5 <- update(f4, . ~ . + log_market_ret + r1y)
summary(f5)
AIC(f1, f2, f3, f4, f5)

#####
# what if we add a `has prior distress` dummy?
f6 <- update(f5, . ~ . + has_prior_distress)
summary(f6)
AIC(f1, f2, f3, f4, f5, f6)

# we will drop it again... It may be low in the start due to the length of
# the Moody's data
local({
  tmp <- tapply(dat$has_prior_distress, dat$tstop, mean)
  plot(make_ym_inv(as.integer(names(tmp))), tmp, type = "l", 
       ylim = range(0, tmp), xlab = "Date", 
       ylab = "Fraction with prior distress")
})

#####
# what about the industry dummies from Chava et al. (2004)
dat$sic_grp <- relevel(factor(dat$sic_grp), "Misc") # use same reference point
f7 <- update(f6, . ~ . + sic_grp - has_prior_distress)
summary(f7)
AIC(f1, f2, f3, f4, f5, f6, f7)

#####
# what about the interactions Chava et al. (2004) argues for
f8 <- update(f7, . ~ . + sic_grp:(wz(r_niq_atq) + wz(r_ltq_atq)))
summary(f8)

# is it significant?
anova(f5, f7, f8, test = "LRT")
AIC(f1, f2, f3, f4, f5, f6, f7, f8)

# we remove these for now
f9 <- update(f8, . ~ . - sic_grp:(wz(r_niq_atq) + wz(r_ltq_atq)) - sic_grp)
summary(f9)
AIC(f1, f2, f3, f4, f5, f6, f7, f8, f9)

#####
# try different size measures
local({
  func <- function(x){
    cat("\nCoef(", deparse(substitute(x)), "):\n", sep = "")
    print(coef(x))
  }
  
  s1 <- update(f9, . ~ . - wz(rel_size) + wz(atq_defl_log ))
  s2 <- update(f9, . ~ . - wz(rel_size) + wz(actq_defl_log))
  s3 <- update(f9, . ~ .                + wz(atq_defl_log ))
  s4 <- update(f9, . ~ .                + wz(actq_defl_log))
  
  func(s1)
  func(s2)
  func(s3)
  func(s4)
  
  AIC(s1, s2, s3, s4, f9)
})

#####
# simplify and add a new size measure
f10 <- update(
  f9, . ~ wz(actq_defl_log) + . - wz(r_req_atq) - wz(r_saleq_atq))
summary(f10)
AIC(f1, f2, f3, f4, f5, f6, f7, f8, f9, f10)

# keep the last model 
f0 <- f10
rm(f1, f2, f3, f4, f5, f6, f7, f8, f9, f10)
```

The effect of current assets may seem odd when `rel_size` is included. Fit 
a GAM to see association.

```{r check_actq_defl_log_effect, cache = 1}
library(mgcv)
library(parallel)
local({
  make_plot_func <- (function(){
    cl <- makeCluster(5L)
    on.exit(stopCluster(cl))
    dat_use <- with(
      dat, data.frame(
        y, rel_size = unclass(wz(rel_size, do_center = FALSE)), 
        actq_defl_log = unclass(wz(actq_defl_log, do_center = FALSE))))
    fit <- bam(y ~ te(rel_size, actq_defl_log, k = c(15, 15), bs = "cs"), 
               binomial("cloglog"), dat_use, cluster = cl)
    print(summary(fit))
    
    library(MASS)
    dens_N <- 50L
    dens <- kde2d(dat_use$rel_size, dat_use$actq_defl_log, n = dens_N)
    dens_small <- kde2d(dat_use$rel_size, dat_use$actq_defl_log, n = dens_N - 1L)
    
    function(){
      # first we make the plot using mgcv
      for(i in 0:3)
        vis.gam(fit, c("rel_size", "actq_defl_log"), theta= 20 + 90 * i, 
                ticktype = "detailed", color = "gray", 
                zlab = "\nLog-hazard", 
                xlab = paste0("\n", get_label("rel_size")), 
                ylab = paste0("\n", get_label("actq_defl_log")))
      
      # show kernel density plot to strees that there is mainly data in some 
      # regions
      library(lattice)
      cols <- colorRampPalette(gray.colors(7, start = .9, end = 0))(40)
      print(lvl <- levelplot(
        dens$z, row.values = dens$x, column.values = dens$y, 
        col.regions = cols, xlab = get_label("rel_size"), 
        ylab = get_label("actq_defl_log"), 
        scales = list(tck = c(1,0)), 
        xlim = range(dens$x), ylim = range(dens$y)))
      
      # then we make a similar 3D plot where we focus on the high density areas
      keep <- dens$z > (max(dens$z) / 66)
      cat(sprintf(
        "%.2f pct. of density is included and %.2f of the squares\n", 
        100 * sum(dens$z[keep]) / sum(dens$z), 100 * mean(keep)))
      image(keep, x = dens$x, y = dens$y)
      
      dat_plot <- expand.grid(rel_size = dens$x, actq_defl_log = dens$y)
      dat_plot <- within(dat_plot, {
        z_plot <- z <- predict(fit, dat_plot, type = "link")
        keep <- c(keep)
        dens = c(dens$z)
        z_plot[!keep] <- NA_real_
      })
      
      # plot subset where we have data
      for(i in 0:3)
        with(dat_plot, {
          x_vals <- unique(rel_size)
          y_vals <- unique(actq_defl_log)
          z_vals <- matrix(z_plot, length(x_vals), length(y_vals))
          
          cols <- dens_small$z
          cols <- level.colors(
            cols, pretty(cols), lvl$panel.args.common$col.regions)
          
          persp(
            x = unique(rel_size), y = unique(actq_defl_log), z = z_vals, 
            ticktype = "detailed", theta= 20 + 90 * i, col = cols, 
            zlab = "\nLog-hazard", 
            xlab = paste0("\n", get_label("rel_size")), 
            ylab = paste0("\n", get_label("actq_defl_log")))
        })
    }
  })()
  
  get_plot_device(make_plot_func(), "marginal-rel_size-actq_defl_log", 
                  onefile = FALSE)
})
```


</div>

## Standardized coefs
Look at standardized coeffcients

<div class="hideable">

```{r check_std_coef}
local({
  fr <- y ~ 
    wz(r_ltq_atq, do_scale = TRUE) + wz(r_niq_atq, do_scale = TRUE) + 
    wz(r_actq_lctq, do_scale = TRUE) + wz(sigma, do_scale = TRUE) + 
    wz(excess_ret, do_scale = TRUE) + wz(rel_size, do_scale = TRUE) +
    wz(dtd, do_scale = TRUE) + scale(r1y) + 
    wz(r_oiadpq_atq, do_scale = TRUE) + wz(r_mv_ltq, do_scale = TRUE) + 
    scale(log_market_ret) + wz(r_wcapq_atq, do_scale = TRUE) + 
    wz(actq_defl_log, do_scale = TRUE)
  stopifnot(setequal(all.vars(fr), all.vars(formula(f0))))
  fit <- glm(fr, binomial("cloglog"), dat)
  stopifnot(AIC(fit) == AIC(f0))
  print(su <- summary(fit))
  coe <- su$coefficients
  print(coe[order(abs(coe[, 1]), decreasing = TRUE), ])
  
  nam <- gsub(
    "^((wz|scale)\\()([a-zA-Z0-9_]+)(,.+|\\))$", "\\3", rownames(coe), perl = TRUE)
  nam[-1] <- sapply(nam[-1], get_label)
  nam[1] <- "Intercept"
  
  rownames(coe) <- nam
  coe <- coe[, c("Estimate", "z value")]
  colnames(coe)[2] <- "Z-stat"
  
  # order by abs size
  coe <- coe[order(abs(coe[, 1]), decreasing = TRUE), ]
  coe[] <- sprintf("% 7.3f", coe)
  
  library(tableHTML)
  write_tableHTML(
    tableHTML(coe), file = file.path(
      "markdown", "output", "glm-std-coef.html"))
  
  print(coe, quote = FALSE)
})
```

</div>

## Variance inflation factor

```{r vif}
library(car)
vif(f0)
```

## Distance-to-Default

First we compare the difference between including the Distance-to-Default 
on the log-hazard scale (complementary log-log) and then we consider using 
the corresponding probabilities from the Merton model as in @Bharath08 on 
the log-hazard scale. 

<div class="hideable">

```{r comp_scales}
get_plot_device({
  ps <- pnorm(seq(-7, 0, length.out = 1000))
  DtD <- -qnorm(ps)
  cll <- function(p)
    log(-log1p(-p))
  x <- cll(ps)
  
  # DtD vs. cloglog
  par(mar = c(4, 5, 1, 1))
  plot(x, DtD, type = "l", 
       xlab = expression(log(-log(1 - p))), 
       ylab = expression(textstyle(DtD) == {Phi^-1}(1 - p)), 
       ylim = rev(range(DtD)))
  # add qqline
  qqline. <- function(ph1, ph2, f1, f2){
    p1 <- c(f1(ph1), f2(ph1))
    p2 <- c(f1(ph2), f2(ph2))
    b <- (p2[2] - p1[2]) / (p2[1] - p1[1])
    a <- p1[2] - b * p1[1]
    abline(a = a, b = b, lty = 2)
  }
  qqline.(.00001, .04, cll, function(p) qnorm(1 - p))
  
  # transformed
  par(mar = c(4, 5, 1, 1))
  plot(x, ps, type = "l", yaxt = "n",
       xlab = expression(log(-log(1 - p))), 
       ylab = expression(p == Phi(-textstyle(DtD))~(textstyle(DtD)~textstyle(shown))))
  qqline.(.00001, .04, cll, identity)
  yat <- seq(0, 7, by = .5)
  axis(2, at = pnorm(1 - yat), labels = yat)
}, "comp-dtd-and-ps-vs-cloglog", onefile = FALSE, set_par = FALSE)
```

</div>

Next, we consider the remark 

> Unreported models that use either the log of market equity or the log of 
> the Merton DD distance to default, rather than the Merton DD probability, 
> perform uniformly worse than the resu

from @Bharath08. I do not get how they can use the log of the Merton 
distance-to-default when it has negativ values [@Bharath08, tabel 1]. Thus, we compare
using the distance to default on the log-hazard scale as in @Duffie07 
versus using the Merton DD probability.

<div class="hideable">

```{r dtd_check, cache = 1}
local({
  # compute the Merton DtD probability
  dat$dtd_prob <- pnorm(-wz(dat$dtd, do_scale = FALSE, do_center = FALSE))
  hist(dat$dtd_prob)
  
  # univariate model
  f_dtd      <- glm(y ~ wz(dtd) , binomial("cloglog"), dat)
  f_prob_dtd <- glm(y ~ dtd_prob, binomial("cloglog"), dat)
  print(AIC(f_dtd, f_prob_dtd))
  
  # model as in Duffie et al. (2007)
  base_f <- glm(
    y ~ wz(excess_ret) + r1y + log_market_ret, binomial("cloglog"), dat)
  f_dtd      <- update(base_f, . ~ . + wz(dtd))
  f_prob_dtd <- update(base_f, . ~ . + dtd_prob)
  print(AIC(f_dtd, f_prob_dtd))
  
  # what if we add the probability to the latter model
  f_dtd_w_prob <- update(f_dtd, . ~ . + dtd_prob)
  print(summary(f_dtd_w_prob))
  anova(f_dtd, f_dtd_w_prob, test = "Chisq")
})
```

</div>

## Residual diagnostics

Look at partial residuals versus the linear predictor and the covariates 
(these may not be approiate when the mean is lower than 0.1)

<div class="hideable">

```{r res_diag, cache = 1}
pear_vs_res(f0)

formals(resid_vs_covar)$type <- "partial"
make_resid_vs_covar_plot <- function(fit){
  vars <- attr(terms(fit), "dataClasses")
  keep <- which(vars %in% "numeric")
  vars <- as.list(attr(terms(fit), "variables"))[keep + 1L]
  
  for(v in vars)
    eval(bquote(resid_vs_covar(fit, .(v))))
}
make_resid_vs_covar_plot(f0)
```

</div>

## Add splines

<div class="hideable">

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "splines", 
      path = paste0(file.path("markdown", "cache", "preliminary"), 
                    .Platform$file.sep))
-->

```{r splines, cache = 1, dependson = c("winz", "res_diag")}
s1 <- update(f0, . ~ . + sp_w_c(r_niq_atq, 3L))
summary(s1)
AIC(f0, s1)
anova(f0, s1, test = "LRT")

# check how the estimated spline looks
plot_sp_w_c(s1, "r_niq_atq", ylab = "Log-hazard term")()

# add spline to sigma -- knot is only chosen as a knot closer towards zero 
# yields a collinear basis function after weighting
s2 <- update(s1, . ~ . + sp_w_c(sigma, 3L))
summary(s2)
AIC(f0, s1, s2)
anova(f0, s1, s2, test = "LRT")

plot_sp_w_c(s2, "r_niq_atq", ylab = "Log-hazard term")()
plot_sp_w_c(s2, "sigma"    , ylab = "Log-hazard term")()

s3 <- update(s2, . ~ . - wz(r_mv_ltq) + wz(r_mv_ltq_log) + 
               sp_w_c(r_mv_ltq_log, 3L))
summary(s3)

AIC(f0, s1, s2, s3)
anova(f0, s1, s2, s3, test = "LRT")

# plot
get_plot_device({
  plot_sp_w_c(s3, "r_niq_atq"   , ylab = "Log-hazard term")()
  plot_sp_w_c(s3, "sigma"       , ylab = "Log-hazard term")()
  plot_sp_w_c(s3, "r_mv_ltq_log", ylab = "Log-hazard term")()
}, "spline_plots", onefile = FALSE)

# check residual plots
make_resid_vs_covar_plot(s3)
pear_vs_res(s3)

s0 <- s3
rm(s1, s2, s3)
```

</div>

Are the effects similar to the marginal effects?

<div class="hideable">

```{r gam_mod, cache = 1}
library(mgcv)
local({
  . <- function(x){
    x <- substitute(x)
    qs <- eval(bquote(quantile(.(x), probs = c(.01, .99))), dat)
    qs <- as.numeric(formatC(qs, format = "g", digits = 4))
    
    require(parallel)
    cl <- makeCluster(4L)
    on.exit(stopCluster(cl))
    
    fit <- eval(bquote(
      bam(y ~ s(wz(.(x), lb = .(qs[1]), ub = .(qs[2])), k = 30L, bs = "cr"), 
          binomial("cloglog"), dat, cluster = cl)))
    print(summary(fit))
    
    function(){
      plot(fit, xlab = get_label(deparse(x)), ylab = "Linear predictor term")
      add_hist(eval(bquote(wz(.(x))), dat))
    }
  }
  plot_list <- list(.(r_niq_atq), .(sigma), .(r_mv_ltq_log))
  
  get_plot_device({
    for(i in plot_list)
      i()
  }, "marginal_spline_plots", onefile = FALSE)
})
```

</div>

Check for interactions

<div class="hideable">

```{r check_interact, cache = 1, dependson = "res_diag", fig.height = 8, fig.width = 8}
# do plots
vars <- list(
  r_req_atq    = list(quote(wz(r_niq_atq))),
  r_req_atq    = list(quote(wz(sigma))),
  rel_size    = list(quote(wz(excess_ret))),
  r_ltq_atq    = list(quote(wz(dtd))),
  # due to Lando et al. (2013)
  actq_lctq   = list(quote(wz(r_actq_lctq))))
for(i in seq_along(vars)){
  for(j in seq_len(length(vars) - i) + i){
    cl <- list(
      quote(resid_vs_covar_inter), quote(s0), vars[[i]][[1]],
      vars[[j]][[1]], type = "partial")
    eval(as.call(cl))
  }
}
```


</div>

## Add interactions

Add interactions (especially those similar to @Lando13)

<div class="hideable">

<!--
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
    "add_inter", 
    path = paste0(file.path("markdown", "cache", "preliminary"), 
                  .Platform$file.sep))
-->

```{r add_inter, cache = 1, dependson = "splines"}
i1 <- update(s0, . ~ . + wz(r_actq_lctq) : wz(dtd)) 
summary(i1)
AIC(s0, i1)
anova(s0, i1, test = "LRT")

i2 <- update(i1, . ~ . + wz(r_niq_atq) : wz(dtd)) 
summary(i2)
AIC(s0, i1, i2)
anova(s0, i1, i2, test = "LRT")

i3 <- update(i2, . ~ . + wz(r_actq_lctq) : wz(sigma))
summary(i3)
AIC(s0, i1, i2, i3)
anova(s0, i1, i2, i3, test = "LRT")

# simplify model due to many test
i0 <- update(i3, . ~ . - wz(r_niq_atq) : wz(dtd) - wz(r_actq_lctq):wz(dtd))

# make plots
pear_vs_res(i0)

get_plot_device({  
  plot_sp_w_c(i0, "r_niq_atq", ylab = "Log-hazard term")()
  plot_sp_w_c(i0, "sigma", ylab = "Log-hazard term")()
  plot_sp_w_c(i0, "r_mv_ltq_log", ylab = "Log-hazard term")()
}, "spline_plots_w_interact", onefile = FALSE)

rm(i1, i2, i3) 
```

```{r redo_check_interact, cache = 1, dependson = c("add_inter", "check_interact"), fig.height = 8, fig.width = 8}
for(i in seq_along(vars)){
  for(j in seq_len(length(vars) - i) + i){
    cl <- list(
      quote(resid_vs_covar_inter), quote(i0), vars[[i]][[1]],
      vars[[j]][[1]], type = "partial")
    eval(as.call(cl))
  }
}
```

</div>

## Influential observations

There are some observations that may need an extra check

<div class="hideable">

```{r needs_extra}
local({
  resids <- residuals(i0, "deviance")
  vs <- c("tstop", "gvkey", "distress_type", all.vars(formula(i0)))
  
  cat("Remember the coefficient estimates\n")
  print(coef(i0))
  
  keep <- abs(resids) > 1
  tr <- predict(i0, type = "terms")[keep, ]
  o <- merge(
    cbind(dat[, vs], resids = resids)[keep, ], 
    with(dat, data.frame(tstop, gvkey, mast_issr_num, conm, 
                         date = make_ym_inv(tstop))), 
    by = c("gvkey", "tstop"))
  tr <- tail(tr[order(abs(o$resids)), ], 5)
  o  <- tail(o [order(abs(o$resids)), ], 5)
  rownames(o) <- nrow(o):1
  cat("Covs\n")
  print(o)
  
  cat("\nTerms\n")
  print(tr)
})
```

</div>

<div class="hideable">

```{r check_influence, cache = 1, dependson = "splines"}
# test significance of terms
(dr_i0 <- drop1(i0, test = "LRT"))
```

</div>

Make table with likelihood ratio tests

<div class="hideable">

```{r lrt_test_tbl}
local({
  lrt_tb <- dr_i0
  
  # change row names
  nam <- rownames(lrt_tb)
  nam[1] <- "Full model"

  regexp <- "(wz\\()([a-zA-Z0-9_]+)(,.+|\\))"
  mas <- grepl(regexp, nam, perl = TRUE)
  nam[mas] <- sapply(gsub(regexp, "\\2", nam[mas], perl = TRUE), get_label)
  
  regexp <- "(sp_w_c\\()([a-zA-Z0-9_]+)(,.+|\\))"
  mas <- grepl(regexp, nam, perl = TRUE)
  nam[mas] <- paste(
    sapply(gsub(regexp, "\\2", nam[mas], perl = TRUE), get_label), 
    "(spline term)")
  
  regexp <- "^log_market_ret|r1y$"
  mas <- grepl(regexp, nam, perl = TRUE)
  nam[mas] <- sapply(nam[mas], get_label)
  
  nam[-1] <- paste("%", nam[-1])
  
  rownames(lrt_tb) <- nam
  
  # pick the columns we need, order, and print before turning to chars
  lrt_tb <- lrt_tb[, c("AIC", "Df", "LRT", "Pr(>Chi)")]
  lrt_tb <- lrt_tb[c(1, order(lrt_tb[-1, "AIC"]) + 1L), ]
  print(lrt_tb)
  
  # turn to chars for better format
  pvals <- lrt_tb[, "Pr(>Chi)"]
  x <- sprintf("  %0.6f", pvals)
  x[pvals < 1e-6] <- "< 0.000001"
  
  lrt_tb_fn <- cbind(
    AIC = sprintf("%.2f", lrt_tb$AIC),
    Df = sprintf("%d", lrt_tb$Df), 
    `LR-stat` = sprintf("%6.2f", lrt_tb$LRT), 
    `P-value` = x)
  
  lrt_tb_fn[1, -1] <- ""
  rownames(lrt_tb_fn) <- rownames(lrt_tb)
  
  write_tableHTML(
    tableHTML(lrt_tb_fn), file = file.path(
      "markdown", "output", "glm-drop-one-tab.html"))
  
  print(lrt_tb_fn, quote = FALSE)
})
```

</div>

## Add industry dummies

<div class="hideable">

```{r re_add_indu, cache = 1, dependson = "splines"}
resid_vs_covar_inter(i0, sic_grp, tstop, bw = 4, type = "partial")
resid_vs_covar_inter(
  i0, as.factor(sic_grp_fine), tstop, bw = 4, type = "partial")

h1 <- update(i0, . ~ . + sic_grp)
summary(h1)
anova(i0, h1, test = "LRT")

# more fine grained industry groups
h2 <- update(i0, . ~ . + as.factor(sic_grp_fine))
summary(h2)
anova(i0, h1, h2, test = "LRT")

rm(h1, h2)
```

</div>

## Check plot against time

<div class="hideable">

```{r check_time}
# assumes that all rows represent one month
resid_vs_covar(i0, tstop, bw = 4, type = "partial")
resid_vs_covar_inter(i0, sic_grp, tstop, bw = 4, type = "partial")
```

```{r check_time_two, fig.height=8, fig.width=8}
if(!any(sapply(vars, function(x) x[[1]] == quote(wz(rel_size)))))
  # add variable due to size effect found in Lando et al. (2013)
  vars <- c(vars, list(list(quote(wz(rel_size)))))

for(i in seq_along(vars)){
  cl <- list(
    quote(resid_vs_covar_inter), quote(i0), quote(tstop),
    vars[[i]][[1]], n_grp = 18L, type = "partial")
  eval(as.call(cl))
}
```

</div>

## In-sample fit

Fit model from  @Duffie09 without the random effect (i.e., like @Duffie07)

<div class="hideable">

```{r fit_duf, cache = 1}
duf <- glm(
  y ~ wz(dtd) + wz(excess_ret) + r1y + log_market_ret, 
  binomial("cloglog"), dat)
summary(duf)

# standardized as in the article
local({
  duf_scale <- glm(
  y ~ wz(dtd, do_scale = TRUE) + wz(excess_ret, do_scale = TRUE) + 
    scale(r1y) + scale(log_market_ret), binomial("cloglog"), dat)
  summary(duf_scale) # compare w/ table III on page 2103
})
```

</div>

Create coefficient table

<div class="hideable">

```{r get_coef_table_input}
coef_tbl_input <- lapply(
  list(duffie = duf, only_lin = f0, final = i0), 
  function(x){
    tr = terms(x)
    list(
      coef = coef(x), 
      vcov = vcov(x),
      terms = tr, 
      logLik = logLik(x),
      AIC = AIC(x))
})
```

```{r make_coef_table}
all_coefs <- unique(unlist(
  sapply(coef_tbl_input, function(x) attr(x$terms, "term.labels"))))
all_coefs <- c("(Intercept)", all_coefs)

coef_tbl <- lapply(coef_tbl_input, function(x){
  # create table to store output
  out <- matrix(
    NA_real_, nrow = length(all_coefs), ncol = 3, 
    dimnames = list(all_coefs, c(
      "Estimate", "Test statistic", "p-value")))
  
  # find matching columns
  stopifnot("(Intercept)" %in% names(x$coef))
  labs <- c("(Intercept)", attr(x$terms, "term.labels"))
  ma <- match(labs, all_coefs)
  stopifnot(!anyNA(ma))
  
  # insert coefficients
  asg <- attr(model.matrix(x$terms, dat), "assign") + 1L
  mult_col <- logical(length(ma))
  mult_col[unique(asg[duplicated(asg)])] <- TRUE
  mult_col[1L] <- FALSE
  out[ma[!mult_col], "Estimate"] <- x$coef[!asg %in% which(mult_col)] 
  
  # make Wald test and add test statistics and p-values
  dfs <- table(asg)
  test_stat <- mapply(
    function(b, cv) drop(b %*% solve(cv, b)), 
    b = split(x$coef, asg), 
    cv = lapply(unique(asg), function(i){
      idx <- which(i == asg) 
      x$vcov[idx, idx, drop = FALSE]
    }))
  out[ma, c("Test statistic", "p-value")] <- cbind(
    test_stat, pchisq(test_stat, dfs, lower.tail = FALSE))
  
  # Add AIC and log-likelihood 
  out <- rbind(
    out, 
    AIC              = c(x$AIC   , NA_real_, NA_real_), 
    `log-likelihood` = c(x$logLik, NA_real_, NA_real_), 
    `# firms`        = c(length(unique(dat$gvkey)), NA_real_, NA_real_))
  
  out
})

# printed to be able to compare with latex result below
print(do.call(cbind, coef_tbl), na.print = "")

#####
# make latex table
local({
  nr <- length(all_coefs)
  nm <- length(coef_tbl)
  nc <- nm * 2L + 1L
  
  t_arg <- paste(
    "S[table-format=-2.3 ,table-alignment=right]@{}", 
    "@{}l", 
    "S[table-format=2.3,table-space-text-pre={*}, table-space-text-post={-*}]", 
    sep = "\n")
  cmark <- "{\\makecell[r]{\\checkmark}}"
  
  #####
  # header
  cat(
    "\\begin{tabular}{l\n", 
    paste(rep(t_arg, nm), sep = "\n"), "}\n", sep = "", 
    "\\toprule\n", 
    "& ", paste0("\\multicolumn{3}{c}{$\\mathcal{M}_", 
                 1:nm, "$}", collapse = " & "), "\\\\\n", 
    "\\midrule\n")
  
  #####
  # coefficient estimate and test statistics
  get_pstart <- function(x, p){
    s <- ifelse(
      p > .1 | is.na(p), "", ifelse(
        p > .05, "$^{*}$", ifelse(
          p > .01, "$^{**}$", "$^{***}$")))
    
    ifelse(is.na(x), " & ", paste0(x, " & ",  s))
  }
  for(i in 1:nr){
    # row name
    na <- rownames(coef_tbl[[1L]])[i]
    na <- if(na == "(Intercept)") "Intercept" else {
      na <- gsub(
        "(wz\\()([a-zA-Z0-9_]+)(,.+|\\))", "\\2", na, perl = TRUE)
      regexp <- "(sp_w_c\\()([a-zA-Z0-9_]+)(,.+|\\))"
      is_spline <- grepl(regexp, na, perl = TRUE)
      na <- gsub(regexp, "\\2", na, perl = TRUE)
      
      if(is_spline) 
        paste0(get_label(na), " (spline)") else 
          get_label(na)
    }
    na <- gsub("\\*", "$\\\\cdot$", na)
    
    cat(na, "& ")
    
    # coefficients
    z <- sapply(coef_tbl, "[", i = i, j = TRUE)
    
    z_t <- z["Test statistic", ]
    z_t <- ifelse(is.na(z_t), "", sprintf("(%.1f)", z_t))
    
    z_e <- z["Estimate", ]
    z_e <- ifelse(!is.na(z_e), sprintf("%.3f", z_e),  
                  ifelse(z_t != "", cmark, ""))
    z_e <- get_pstart(z_e, z["p-value", ])
    
    cat(rbind(z_e, z_t), sep = " & ")
    cat(" \\\\\n")
  }
  
  cat("\\midrule\n")
  
  #####  
  # AIC, log-likelihood etc. 
  z <- sprintf("%.1f", sapply(coef_tbl, "[", i = "AIC", j = 1), 1)
  cat("AIC & ", paste0(z, " & & ", collapse = " & "), "\\\\\n")
  z <- sprintf("%.1f", sapply(coef_tbl, "[", i = "log-likelihood", j = 1), 1)
  cat("log-likelihood & ", paste0(z, "  & & ", collapse = " & "), "\\\\\n")
  
  z <- sprintf("%d", sapply(coef_tbl, "[", i = "# firms", j = 1), 1)
  cat("Number of firms & ", paste0(z, " & & ", collapse = " & "), "\\\\\n")
  
  cat("\\bottomrule\n\\end{tabular}")
})
```

</div>

Show in-sample results

<div class="hideable">

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "in_sample", 
      path = paste0(file.path("markdown", "cache", "preliminary"), 
                    .Platform$file.sep))
-->

```{r in_sample, cache = 1}
do_sims <- function(phat, tvar, nsim = 10000, spec = 6){
  library(parallel)
  cl <- makeCluster(spec)
  on.exit(stopCluster(cl))
  clusterSetRNGStream(cl)
  
  sims <- parSapply(cl, 1:nsim, function(..., tvar, phat){
    y <- phat > runif(length(phat))
    tapply(y, tvar, mean)
  }, tvar = tvar, phat = phat)
  
  lbs <- apply(sims, 1, quantile, probs = .05)
  ubs <- apply(sims, 1, quantile, probs = .95)
  
  list(lbs = lbs, ubs = ubs, mea = tapply(phat, tvar, mean))
}

set.seed(85664431)
i0_sims <- do_sims(
  phat = predict(i0, type = "response", newdata = dat), 
  tvar = dat$tstop)

f0_sims <- do_sims(
  phat = predict(f0, type = "response", newdata = dat), 
  tvar = dat$tstop)

duf_sims <- do_sims(
  phat = predict(duf, type = "response", newdata = dat), 
  tvar = dat$tstop)
```

```{r plot_in_sample_sims, dependson = c("fit_duf", "splines")}
make_sims_plot <- function(sims, dat, ylim = NA){
  require(lubridate)
  rea <- tapply(dat$y, dat$tstop, mean)
  x <- make_ym_inv(as.integer(names(rea)))
  is_miss <- rea < sims$lbs | rea > sims$ubs
  plot(rea ~ x, pch = 16, ylim = ylim, 
       col = ifelse(is_miss, "Black", "DarkGray"), cex = .7, 
       xlab = "Year", ylab = "Average/realised distress rate", 
       xaxt = "n")
  nber_poly(is_dates = TRUE, very_ligth_gray = TRUE)
  min_d <- as.POSIXlt(min(x))
  month(min_d) <- 1L
  max_d <- as.POSIXlt(max(x))
  ax <- min_d
  while(tail(ax, 1)$year + 1L <= max_d$year){
    new_ax <- tail(ax, 1)
    new_ax$year <- new_ax$year + 1L
    ax <-  c(ax, new_ax)
  }
  labs <- format(ax, "%Y")
  ix <- seq_along(labs)
  labs[ix %% 2 == 0] <- ""
  axis(side = 1, at = as.Date(ax), labels = labs, las = 2)
  
  lines(make_ym_inv(as.integer(names(rea))), sims$mea)
  polygon(c(x, rev(x)), c(sims$lbs, rev(sims$ubs)), 
          col = rgb(0, 0, 0, .1), border = NA)
  
  # diff plot
  plot(rea - sims$mea ~ x,  
       col = ifelse(is_miss, "Black", "DarkGray"), cex = .7, 
       xlab = "Year", ylab = "Difference from realised default rate", 
       xaxt = "n", type = "h", ylim = c(-diff(ylim) * .4, diff(ylim) * .5))
  nber_poly(is_dates = TRUE, very_ligth_gray = TRUE)
  axis(side = 1, at = as.Date(ax), labels = labs, las = 2)
}

get_plot_device(
  make_sims_plot(i0_sims, dat, c(0, .014)), 
  "glm-spline-agg-in-sample", onefile = FALSE)
get_plot_device(
  make_sims_plot(f0_sims , dat, c(0, .014)), 
  "glm-no-spline-agg-in-sample", onefile = FALSE)
get_plot_device(
  make_sims_plot(duf_sims, dat, c(0, .014)), 
  "glm-duf-agg-in-sample", onefile = FALSE)
```

```{r auc_in_sample}
library(pROC)
auc(response = dat$y, predictor = predict(i0 , type = "response"))
auc(response = dat$y, predictor = predict(f0 , type = "response"))
auc(response = dat$y, predictor = predict(duf, type = "response"))
```

</div>

## Out-sample fit

<div class="hideable">

<!--
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "fit_out_sample", 
      path = paste0(file.path("markdown", "cache", "preliminary"), 
                    .Platform$file.sep))
-->

```{r fit_out_sample, cache = 1, dependson = c("fit_duf", "splines")}
yrs <- 1999:as.integer(format(make_ym_inv(max(dat$tstop)), "%Y"))
dat$year <- as.integer(format(make_ym_inv(dat$tstop), "%Y"))

out <- cbind(
  data.frame(y = dat$y), year = dat$year, 
  gvkey = dat$gvkey, tstop = dat$tstop, f0_pred = NA_real_, 
  i0_pred = NA_real_, duf_pred = NA_real_)

frms <- list(f = formula(f0), s = formula(i0), d = formula(duf))
out_sample_res <- lapply(yrs, function(yr){
  cat("Running", sQuote(yr), "\n")
  
  fits <- lapply(
    frms, glm, family = binomial("cloglog"), dat = subset(dat, year < yr))
  idx <- dat$year == yr
  
  preds <- lapply(fits, predict, newdata = dat[idx, ], type = "response")
  
  res <- sapply(preds, function(x){
    c(auc         = c(auc(response = dat$y[idx], x)), 
      `log score` = -mean(ifelse(dat$y[idx], log(x), log(1 - x))))
  })

  out[idx, "f0_pred"] <<- preds$f
  out[idx, "i0_pred"] <<- preds$s
  out[idx, "duf_pred"] <<- preds$d
  
  list(res = res, coefs = lapply(fits, function(x) summary(x)$coefficients))
})
```

```{r plot_out_sample_metrics}
metrics <- simplify2array(lapply(out_sample_res, "[[", "res"))
metrics <- metrics[, c("f", "s", "d"), ]

get_plot_device({
  matplot(
    yrs, t(metrics["auc", , ]), type = "p", pch = 16:18, 
    ylab = "Out-of-sample AUC", xlab = "Year")
}, "glm-auc-out-sample")

get_plot_device({
  matplot(
    yrs, t(metrics["log score", , ]), type = "p", pch = 16:18, 
    ylab = "Out-of-sample log-score", xlab = "Year")
}, "glm-log-score-out-sample")

get_plot_device({
  matplot(
    yrs, cbind(
      metrics["log score", "f", ] - metrics["log score", "d", ], 
      metrics["log score", "s", ] - metrics["log score", "d", ]), 
    pch = 16:17, col = 1:2, 
    ylab = "Out-of-sample log-score difference", xlab = "Year")
  abline(h = 0, lty = 2)
}, "glm-log-score-diff-out-sample")
```

<!--
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "sim_out_sample", 
      path = paste0(file.path("markdown", "cache", "preliminary"), 
                    .Platform$file.sep))
-->

```{r sim_out_sample, cache=1, dependson= c("in_sample", "fit_out_sample")}
min_yr <- min(yrs) 
stopifnot(
  all(is.na(subset(out, year < min_yr, 
                   c("f0_pred", "i0_pred", "duf_pred")))),
  !anyNA(subset(out, year >= min_yr, 
                c("f0_pred", "i0_pred", "duf_pred"))))

out <- subset(out, year >= min_yr)
set.seed(85664431)
i0_sims_out <- do_sims(phat = out[, "i0_pred"], tvar = out$tstop)
f0_sims_out <- do_sims(phat = out[, "f0_pred"], tvar = out$tstop)
duf_sims_out <- do_sims(phat = out[, "duf_pred"], tvar = out$tstop)
``` 

```{r plot_out_sample_sims}
get_plot_device(
  make_sims_plot(i0_sims_out , out, c(0, .011)), onefile = FALSE,
  "glm-spline-agg-out-sample")
get_plot_device(
  make_sims_plot(f0_sims_out , out, c(0, .011)), onefile = FALSE, 
  "glm-no-spline-agg-out-sample")
get_plot_device(
  make_sims_plot(duf_sims_out, out, c(0, .011)),  onefile = FALSE,
  "glm-duf-agg-out-sample")
```

</div>

## References