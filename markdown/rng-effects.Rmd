---
title: "Model with random effects"
author: 
  - Rastin Matin
  - Benjamin Christoffersen 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:  
  html_document:
    toc: true
bibliography: refs.bib
---

<script>
$(document).ready(function(){
  var hide_divs = $("div.hideable");
  hide_divs.each(function(){
    // Wrap content in div
    $(this).wrapInner( "<div class='hideable_content', style='display: none;'></div>");
    
    // Add button
    $(this).prepend("<button id='toogle'>show</button>");
  });
  
  // Add hideable btn
  // Put the rest in a div
  
  $("div.hideable button#toogle").click(function(){
    var parent = $(this).parent();
    var target_div = $(parent).find("div.hideable_content");
    
    if(target_div.css("display") == "none"){
      target_div.show();
        $(this).text("Hide");
    } else {
      target_div.hide();
      $(this).text("Show");
    }
  });
});
</script>

## Load data

```{r static_setup, include=FALSE, cache=FALSE}
# please do not set options here that could change...
knitr::opts_chunk$set(
  cache.path = 
    paste0(file.path("cache", "rng-effects"), .Platform$file.sep), 
  fig.path = 
    paste0(file.path("fig"  , "rng-effects"), .Platform$file.sep))
```

```{r setup, include=FALSE, cache=FALSE}
# please do set options here that could change...
knitr::opts_chunk$set(
  echo = TRUE, fig.height = 4, fig.width = 7, dpi = 72, comment = "#R", 
  error = FALSE)
options(digits = 4, scipen = 10, width = 90)
```

```{r run_setup, child = 'setup.Rmd'}
```

## Model without random effects

Fit model without random effects

<div class="hideable">

```{r ass_frm_dd, cache = 1}
frm <- y ~ wz(actq_defl_log) + wz(r_wcapq_atq) + wz(r_oiadpq_atq) + 
  wz(r_niq_atq) + wz(r_ltq_atq) + wz(r_actq_lctq) + wz(sigma) + 
  wz(excess_ret) + wz(rel_size) + wz(dtd) + log_market_ret + 
  r1y + sp_w_c(r_niq_atq, 3) + sp_w_c(sigma, 3) + wz(r_mv_ltq_log) + 
  sp_w_c(r_mv_ltq_log, 3) + wz(r_actq_lctq):wz(sigma)
```


```{r fit_no_rng}
# fit model without random effects
base_fit <- glm(frm, binomial("cloglog"), dat)
summary(base_fit)
logLik(base_fit)
```

</div>

Fit models with varying coefficient 

<div class="hideable">

```{r check_time_var_rel_size, cache = 1}
# run regression
library(splines)
local({
  plot_var_coef <- function(var, fit){
    # get spline basis
    tr <- fit$terms
    tstart_spline_idx <- which(
      grepl("^ns\\(tstart", names(attr(tr,"dataClasses"))))
    stopifnot(length(tstart_spline_idx) == 1L)
    x <- data.frame(tstart = sort(unique(fit$data$tstart)))
    x_basis <- eval(
      attr(tr, "predvars")[c(1L, tstart_spline_idx + 1L)], x)[[1L]]
    
    # get coefficients
    var_expr <- deparse(substitute(var))
    idx_term <- which(grepl(paste0("(?<!sp_w_c\\()", var_expr), 
                            attr(tr, "term.labels"), perl = TRUE))
    stopifnot(length(idx_term) == 1L)
    
    asg <- model.matrix(tr, fit$data[1, ])
    idx_coef <- which(attr(asg, "assign") == idx_term)
    co <- coef(fit)[idx_coef]
    
    # plot
    y <- x_basis %*% co
    se <- diag(tcrossprod(x_basis %*% vcov(fit)[idx_coef, idx_coef], x_basis))
    se <- sqrt(se)
    lb <- y - 1.96 * se
    ub <- y + 1.96 * se
    matplot(x, cbind(lb, y, ub), lty = c(3, 1, 3), type = "l", col = "black", 
            xlab = "Time", ylab = paste("Coef", var_expr))
  }
  
  t0 <- update(base_fit, . ~ . + ns(tstart, df = 6, intercept = TRUE) - 1L)
  print(summary(t0))
  
  up <- function(fit, x){
    fit <- substitute(fit)
    x <- substitute(x)
    eval(bquote(
      update(.(fit), . ~ . - .(x) + 
               .(x) : ns(tstart, df = 6, intercept = TRUE))), 
      parent.frame()) 
  }
  t1 <- up(t0, wz(rel_size))
  print(summary(t1))
  plot_var_coef(rel_size, t1)
  
  # use different size measure
  t1.a <- up(t0, wz(actq_defl_log))
  summary(t1.a)
  plot_var_coef(actq_defl_log, t1.a)
  
  t2 <- up(t1, wz(r_niq_atq))
  print(summary(t2))
  plot_var_coef(r_niq_atq, t2)
  
  # industry effects
  t3 <- up(t2, sic_grp)
  print(summary(t3))
  
  print(anova(base_fit, t0, t1.a,         test = "LRT"))
  print(anova(base_fit, t0, t1  , t2, t3, test = "LRT"))
})
```

</div>

```{r check_rebuild_extra, include = FALSE}
if(!interactive()){
  .check_before_merge <- file.path("markdown", "cache", "rng_check")
  if(!file.exists(.check_before_merge)){
    knitr::opts_chunk$set(cache.rebuild = TRUE)
  } else
    knitr::opts_chunk$set(
      cache.rebuild = knitr::opts_chunk$get("cache.rebuild") ||
        !readRDS(.check_before_merge) == digest::digest(dat))
  
  saveRDS(digest::digest(dat), .check_before_merge)
}
```

## State space models 

### Simple model

Fit model as in @Duffie09

```{r load_dynamhaz}
library(dynamichazard)
```

```{r set_n_threads}
# set number of threads
n_threads <- 6L
```

```{r set_ctrl_default}
ctrl_default <- PF_control(
  N_fw_n_bw = 200L, N_smooth = 400L, N_first = 1000L, n_max = 1000, 
  n_threads = n_threads, nu = 8L, covar_fac = 1.2, ftol_rel = 1e-6,
  smoother = "Fearnhead_O_N", eps = 1e-4, averaging_start = 200L)
```

```{r assign_log_n_eval}
# function to sink output to log file
log_n_eval <- function(expr, log_prefix){
  # setup directories and files to keep track of output
  f_name <- paste0("rng-effects", log_prefix, "_", format(Sys.Date()))
  f_dir <- file.path("markdown", "logs", paste0(f_name))
  sink(    file.path("markdown", "logs", paste0(f_name, ".log")))
  dir.create(tmp_dir <- tempfile())
  png(file.path(tmp_dir, "Rplot%04d.png"))
  
  # clean-up
  on.exit({
    sink()
    dev.off()
    try({
      if(!dir.exists(f_dir))
        dir.create(f_dir)
      if(length(new_files <- list.files(tmp_dir, full.names = TRUE)) > 0)
        file.copy(new_files, f_dir, overwrite = TRUE)
      unlink(tmp_dir, recursive = TRUE)
    })
    rm(f_dir, f_name, new_files)
  })
  
  # make call
  eval(substitute(expr), parent.frame())
}
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "fit_duffie_09", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

<div class="hideable">

```{r fit_duffie_09, cache = 1}
set.seed(seed <- 96092198)
log_n_eval({
  duffie_09 <- PF_EM(
    fixed = Surv(tstart, tstop, y) ~ wz(dtd) + wz(excess_ret) + r1y + 
      log_market_ret, 
    random = ~ 1, data = dat, Fmat = diag(.5, 1),
    model = "cloglog", by = 1L, max_T = max(dat$tstop), id = dat$gvkey_unique, 
    Q_0 = 2^2, Q = .1^2, type = "VAR", control = ctrl_default, 
    trace = 1L)
}, "_duffie_09")
```

Take more iterations

```{r def_take_xtra, cache = 1}
take_xtra <- function(
  fit, n_max = 1000L, trace = 0L, N_fw_n_bw = 500L, N_smooth = 1000L, 
  N_first = N_fw_n_bw, N_smooth_final = N_smooth, averaging_start = 200L, 
  eps = 1e-5){
  cl <- fit$call
  ctrl <- fit$control
  ctrl[c("N_fw_n_bw", "N_smooth", "N_first", "n_max", "N_smooth_final", 
         "averaging_start", "eps")] <- 
    c(N_fw_n_bw, N_smooth, N_first, n_max, N_smooth_final, averaging_start, 
      eps)
  cl[c("control", "trace")] <- list(ctrl, trace)
  
  . <- function(x, y)
    eval(substitute({
      if(!is.null(fit$x))  
        cl$y <<- fit$x
    }, list(x = substitute(x), 
            y = if(missing(y)) substitute(x) else substitute(y))))
  
  .(a_0)
  .(fixed_effects)
  if(is.null(fit$psi)){
    .(Q)
    .(F, Fmat)
    
  } else {
    .(phi, phi)
    .(psi, psi)
    .(theta, theta)
    .(G, G)
    .(J, J)
    .(K, K) 
  }
  
  cat("Running\n", paste0("  ", deparse(cl), collapse = "\n"), "\n", sep = "")
  
  eval(cl)
}
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "xtra_fit_duffie_09", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r xtra_fit_duffie_09, cache = 1, dependson = "fit_duffie_09"}
set.seed(seed)   
log_n_eval({
  duffie_09_xtra <- take_xtra(duffie_09, trace = 1L, averaging_start = 80L)
}, "_duffie_09_xtra")
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "large_fit_duffie_09", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r large_fit_duffie_09, cache = 1, dependson = "xtra_fit_duffie_09"}
set.seed(seed) 
duffie_09_large <- take_xtra(
  duffie_09_xtra, n_max = 1L, N_fw_n_bw = 500L, N_smooth = 2500L, 
  N_first = 2500L, N_smooth_final = 2500L)
```

</div>

Check the result

<div class="hideable">

```{r check_duffie_res}
#####
# plot smoothed random effect
pf_plot_effects <- function(
  fit, qlvl = pnorm(-1) * 2, ylabs, type = "smoothed_clouds", x, xlim, 
  change_par = TRUE){
  stopifnot(type %in% c("smoothed_clouds", "forward_clouds"))
  dp <- if(type != "smoothed_clouds") -1 else TRUE
  o <- list(
    mean = get_cloud_means(fit, type = type)[dp, , drop = FALSE],
    qs = get_cloud_quantiles(fit, qlvls = c(qlvl/2, 1 - qlvl/2), 
                             type = type)[, , dp, drop = FALSE])
  cat(sprintf("%.2f pct. confidence intervals\n", (1 - qlvl) * 100))
  
  if(missing(x))
    x <- make_ym_inv((min(dat$tstop)):max(dat$tstop))
  stopifnot(nrow(o$mean) == length(x))
  
  if(change_par){
    par_old <- par(no.readonly = TRUE)
    on.exit(par(par_old))
    par(mar = c(5, 4, 1, 1))
  }
  if(missing(xlim))
    xlim <- range(x)
  for(i in 1:ncol(o$mean)){
    me  <- o$mean[, i]
    lbs <- o$qs[1, i, ]
    ubs <- o$qs[2, i, ]
    plot(me ~ x, ylim = range(lbs, ubs, me, na.rm = TRUE), 
         type = "l", xlab = "Year", ylab = ylabs[i], xlim = xlim)
    nber_poly(TRUE)
    lines(x, lbs, lty = 2)
    lines(x, ubs, lty = 2)
    abline(h = 0)
  }
}
pf_plot_effects(duffie_09_large, ylabs = "Intercept")

#####
# plot effective sample size
plot_eff <- function(fit){
  eff <- fit$effective_sample_size$smoothed_clouds[-1]
  x <- make_ym_inv((min(dat$tstart) + 1L):max(dat$tstart))
  stopifnot(length(eff) == length(x))
  
  plot(eff ~ x, type = "h", xlab = "Year", ylab = "Effective sample size", 
       ylim = range(eff, 0))
}
plot_eff(duffie_09)
plot_eff(duffie_09_xtra)
plot_eff(duffie_09_large)

##### 
# plot log-likelihood and estimates
plot(duffie_09_xtra$log_likes)
plot(duffie_09$EM_ests$F ~ sqrt(duffie_09$EM_ests$Q), 
     xlab = expression(sigma), ylab = expression(varphi), type = "l")
points(sqrt(duffie_09$EM_ests$Q), duffie_09$EM_ests$F, pch = 1)

sqrt(duffie_09_large$Q)
duffie_09_large$F
duffie_09_large$fixed_effects

logLik(duffie_09_large)
local({
  # same model without random effects
  fix_only <- glm(y ~ wz(dtd) + wz(excess_ret) + r1y + log_market_ret, 
                  binomial("cloglog"), dat)
  print(AIC(base_fit, fix_only, duffie_09_large), digits = 6)
  print(logLik(fix_only), digits = 6)
})
```

Compare the coefficient estimates with @Duffie09 [table II] but keep in mind 
that it is another sample period, only industrial firms, and 
a 3 month T-bill rate.

<!--
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "check_duffie_ll", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
-->

```{r check_duffie_ll, cache = 1, dependson = "fit_duffie_09_xtra"}
set.seed(seed)
print(duffie_LL <- 
   logLik(PF_forward_filter(duffie_09_xtra, N_fw = 5000L, N_first = 10000L)), 
   digits = 6)
```

</div>

### Model with a random intercept

Fit model

<div class="hideable">

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "state_full_rng_inter", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r dd_fixed, cache = 1, dependson="ass_frm_dd"}
frm_fixed <- update(frm, Surv(tstart, tstop, y) ~ .)
```

```{r state_full_rng_inter, cache = 1}
set.seed(seed)
log_n_eval({
  state_rng_inter <- PF_EM(
    fixed = frm_fixed, random = ~ 1, 
    data = dat, Fmat = diag(.5, 1),
    model = "cloglog", by = 1L, max_T = max(dat$tstop), id = dat$gvkey_unique, 
    Q_0 = 2^2, Q = .1^2, type = "VAR", control = ctrl_default, 
    trace = 1L)
}, "_state_rng_inter")
```

Take more iterations

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "xtra_state_full_rng_inter", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r xtra_state_full_rng_inter, cache = 1, dependson = "state_full_rng_inter"}
set.seed(seed)   
log_n_eval({
  state_rng_inter_xtra <- take_xtra(state_rng_inter, trace = 1L)
}, "_state_rng_inter_xtra")
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "xtra_n_more_state_full_rng_inter", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r xtra_n_more_state_full_rng_inter, cache = 1, dependson = "xtra_state_full_rng_inter"}
state_rng_inter_large <- take_xtra(
  state_rng_inter_xtra, n_max = 1L, N_fw_n_bw = 5000L, 
  N_smooth = 10000L, N_smooth_final = 2500L, N_first = 2500L)
```

</div>

Check the result

<div class="hideable">

```{r res_state_full_rng_inter}
#####
# plot smoothed random effect
get_plot_device(
  pf_plot_effects(state_rng_inter_large, ylabs = "Intercept"), 
  "rng-inter-smooth-state")

#####
# plot effective sample size
plot_eff(state_rng_inter)
plot_eff(state_rng_inter_xtra)
plot_eff(state_rng_inter_large)

#####
# plot approx log-likes
local({
  n <- length(state_rng_inter$log_likes) + 
    length(state_rng_inter_xtra$log_likes)
  with(state_rng_inter, {
    plot(1:length(log_likes), log_likes, xlim = c(1, n), 
         ylim = range(state_rng_inter_xtra$log_likes, log_likes))  
  })
  with(state_rng_inter_xtra, {
    points((n - length(log_likes) + 1L):n, log_likes, xlim = c(1, n), pch = 16)  
  })
})
plot(state_rng_inter_xtra$log_likes)
logLik(state_rng_inter_large)

##### 
# plot estimates
with(state_rng_inter, {
  plot(
    EM_ests$F ~ sqrt(EM_ests$Q), 
    xlab = expression(sigma), ylab = expression(varphi), type = "l")
  points(sqrt(EM_ests$Q), EM_ests$F, pch = 1)
  })

with(state_rng_inter_xtra, {
  plot(
    EM_ests$F ~ sqrt(EM_ests$Q), 
    xlab = expression(sigma), ylab = expression(varphi), type = "l")
  points(sqrt(EM_ests$Q), EM_ests$F, pch = 1)
  })

sqrt(state_rng_inter_large$Q)
state_rng_inter_large$F
state_rng_inter_xtra$F

# do check that these names match!
compare_fix <- function(ddfit, fname, base = base_fit){
  dd_names <- names(ddfit$fixed_effects)
  dd_names <- gsub("^(ddFixed\\()(.+)(\\))(\\d*|TRUE)$", "\\2\\4", dd_names)
  cat("Check these names\n")
  print(rbind(names(coef(base)), dd_names))
  
  print(co <- cbind(
    `base fit` = coef(base),
    `dd fit`   = ddfit$fixed_effects))
  
  # make html table. Start by formating
  co[] <- sprintf("%7.4f", co)
  
  rn <- rownames(co)
  rn <- gsub("(wz\\()([^\\)]+)(\\))", "\\2", rn)
  rn[rn == "(Intercept)"] <- "Intercept"
  is_sp <- grepl("^sp_w_c", rn)
  rn[is_sp] <- gsub("^(sp_w_c\\()([A-z]+)(,.+)$", "\\2", rn[is_sp])
  
  rn[-1] <- sapply(rn[-1], get_label)
  rn[is_sp] <- paste("Spline", rn[is_sp])
  rownames(co) <- rn
  co[is_sp, ] <- ""
  co <- co[!duplicated(rn), ]
  
  colnames(co) <- c("Model without frailty", "Model with frailty")
  print(co)
  
  # save
  require(tableHTML)
  fname <- file.path("markdown", "output", paste0(fname, ".html"))
  cat("\nSaving to", sQuote(fname), "\n")
  write_tableHTML(tableHTML(co), file = fname)
}
compare_fix(state_rng_inter_large, "comp-fix-inter")

plot_sp_w_c <- function(term, ddfit, base = base_fit, xlab, vals_only = FALSE){
  #####
  # find term index
  tt <- ddfit$terms$fixed
  sp_tr <- which(grepl(paste0("^sp_w_c\\(", term), attr(tt, "term.labels")))
  wz_tr <- which(grepl(paste0("^wz\\(", term), attr(tt, "term.labels")))
  stopifnot(length(sp_tr) == 1L, length(wz_tr) == 1L)
  
  #####
  # make dummy data and get model matrix
  x_range <- quantile(eval(parse(text = term), base$data), probs = c(.01, .99))
  x <- seq(x_range[1], x_range[2],length.out = 1000)
  df <- data.frame(x)
  colnames(df) <- term
  
  n_terms <- length(attr(tt, "term.labels"))
  tt <- drop.terms(tt, setdiff(1:n_terms, c(wz_tr, sp_tr)))
  attr(tt, "intercept") <- 0
  M <- model.matrix(tt, df)
  
  cl <- colnames(M)
  l1 <- drop(M %*% ddfit$fixed_effects[cl])
  l2 <- drop(M %*% coef(base)[cl])
  cv <- vcov(base)[cl, cl]
  l2_se <- sqrt(diag(M %*% tcrossprod(cv, M)))
  l2_lb <- l2 - 1.96 * l2_se 
  l2_ub <- l2 + 1.96 * l2_se 
  
  if(vals_only)
    return(list(x = x, rng = l1, base = l2, base_lb = l2_lb, 
                base_ub = l2_ub))
  
  #####
  # plot
  plot(x, l1, type = "l", 
       ylab = "Linear predictor term", xlab = xlab, 
       ylim = range(l1, l2, l2_lb, l2_ub))
  lines(x, l2, lty = 2)
  lines(x, l2_lb, lty = 2, col = "DarkGray")
  lines(x, l2_ub, lty = 2, col = "DarkGray")
}
plot_sp_w_c("sigma"   , state_rng_inter_large, xlab = get_label("sigma"))
plot_sp_w_c("r_niq_atq", state_rng_inter_large, xlab = get_label("r_niq_atq"))
plot_sp_w_c("r_mv_ltq_log", state_rng_inter_large, xlab = get_label("r_mv_ltq_log"))

##### 
# log-likelihood and AIC approximations
logLik(state_rng_inter_large)
AIC(base_fit, duffie_09_xtra, state_rng_inter_large)
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "ll_state_full_rng_inter", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r ll_state_full_rng_inter, cache = 1, dependson = "state_full_rng_inter_xtra"}
# get better log-likelihood approximation
set.seed(seed)
print(state_rng_inter_LL <- 
   logLik(PF_forward_filter(state_rng_inter_large, N_fw = 5000L, N_first = 10000L)), 
   digits = 6)
```

</div>

Get approximate negative observed information matrix

<div class="hideable"> 

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "sd_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r sd_rng, cache = 1}
get_approx_info_n_score <- function(fit){
  # First check variance of estimates
  ex <- local({
    set.seed(26217670)
    ex <- replicate(10, {
      tmp <- fit
      rnorm(1)
      tmp$seed <- .Random.seed
      o <- PF_get_score_n_hess(tmp, use_O_n_sq = TRUE)
      o$set_n_particles(N_fw = 1000L, N_first = 5000L)
      o$get_get_score_n_hess()
    }, simplify = FALSE)
    
    z <- sapply(ex, function(x) sqrt(diag(solve(x$observation$neg_obs_info))))
    cat("Approximations\n")
    print(z)
    cat("Means\n")
    print(zm <- rowMeans(z))
    cat("Standard deviations\n")
    print(apply(z, 1, sd))
    
    cat("Comparison with model without random effects\n")
    print(cbind(rng = zm, `non-rng` = sqrt(diag(vcov(base_fit)))))
    
    ex
  })
  
  # then make final run we will use
  o <- PF_get_score_n_hess(fit, use_O_n_sq = TRUE)
  o$set_n_particles(N_fw = 5000L, N_first = 10000L)
  o$get_get_score_n_hess()
}

state_rng_inter_derivs <- get_approx_info_n_score(state_rng_inter_large)
```

```{r show_sd_rng}
solve(state_rng_inter_derivs$state$neg_obs_info)
sqrt(diag(solve(state_rng_inter_derivs$state$neg_obs_info)))

sqrt(diag(solve(state_rng_inter_derivs$observation$neg_obs_info)))
```

</div>

### Model with a random relative size slope

Fit model

<div class="hideable">

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "w_size_state_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r w_size_state_rng, cache = 1, dependson = "dd_fixed"}
set.seed(seed)
log_n_eval({
  state_rng_size_restrict <- PF_EM(
    fixed = frm_fixed, random = ~ 1 + wz(rel_size),  
    data = dat, model = "cloglog", by = 1L, max_T = max(dat$tstop), 
    id = dat$gvkey_unique, type = "VAR",  Q_0 = diag(c(1, 2)^2),
    
    J = diag(1, 2), psi = log(c(0.1272, .05)),
    G = matrix(c(1, 0, 
                 0, 0, 
                 0, 0, 
                 0, 1), byrow = TRUE, ncol = 2), theta = c(.9, .9), 
    K = matrix(1, 1, 1), phi = 0,
    
    control = ctrl_default, 
    trace = 1L)
}, "_state_rng_size_restrict")
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "xtra_w_size_state_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r xtra_w_size_state_rng, cache = 1, dependson = "w_size_state_rng"}
set.seed(seed)
log_n_eval({
  state_rng_size_restrict_xtra <- take_xtra(
    state_rng_size_restrict, trace = 1L)
}, "_state_rng_inter_xtra_w_size_one")
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "large_w_size_state_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r large_w_size_state_rng, cache = 1, dependson = "xtra_w_size_state_rng"}
state_rng_size_restrict_large <- take_xtra(
  state_rng_size_restrict_xtra, n_max = 1L, N_fw_n_bw = 10000L, 
  N_smooth = 50000L, N_first = 10000L, N_smooth_final = 2000L)
```

</div>

Check results

<div class="hideable"> 

```{r show_xtra_w_size_state_rng}
#####
# approx log-likelihood
local({
  n <- length(state_rng_size_restrict$log_likes) + 
    length(state_rng_size_restrict_xtra$log_likes)
  with(state_rng_size_restrict, {
    plot(1:length(log_likes), log_likes, xlim = c(1, n), 
         ylim = range(state_rng_size_restrict_xtra$log_likes, log_likes))  
  })
  with(state_rng_size_restrict_xtra, {
    points((n - length(log_likes) + 1L):n, log_likes, xlim = c(1, n), pch = 16)  
  })
})

plot(state_rng_size_restrict_xtra$log_likes)
logLik(state_rng_size_restrict_large)

#####
# estimates
state_rng_size_restrict_xtra$EM_ests
state_rng_size_restrict_large[c("F", "Q", "theta", "psi", "phi")]
sqrt(diag(state_rng_size_restrict_large$Q))
cov2cor(state_rng_size_restrict_large$Q)
state_rng_size_restrict_large$F

get_plot_device(
  pf_plot_effects(state_rng_size_restrict_large, 
                  ylabs = c("Intercept", get_label("rel_size"))), 
  "rng-inter-smooth-state-w-size", onefile = FALSE)

compare_fix(state_rng_size_restrict_large, "comp-fix-inter-rel-size")

# show T-bill rate plot due to changed slope
local({
  tr <- tapply(dat$r1y, dat$tstop, unique)
  stopifnot(length(dim(tr)) == 1L)
  ti <- make_ym_inv(as.integer(names(tr)))
  
  plot(ti, tr, type = "l")
})

plot_sp_w_c(
  "sigma"   , state_rng_size_restrict_large, xlab = get_label("sigma"))
plot_sp_w_c(
  "r_niq_atq", state_rng_size_restrict_large, xlab = get_label("r_niq_atq"))
plot_sp_w_c(
  "r_mv_ltq_log", state_rng_size_restrict_large, xlab = 
    get_label("r_mv_ltq_log"))

#####
# effective sample size and AIC
plot_eff(state_rng_size_restrict)
plot_eff(state_rng_size_restrict_xtra)
plot_eff(state_rng_size_restrict_large)

AIC(base_fit, duffie_09_xtra, state_rng_inter_large, 
    state_rng_size_restrict_large)
```

```{r marg_size_effect, cache = 1}
#####
# get idea of marginal effect
local({
  library(mgcv)
  library(parallel)
  cl <- makeCluster(6L)
  dat$wz_rel_size <- wz(dat$rel_size)
  dat$ti_var <- make_ym_inv(dat$tstop)
  dat$ti_var <- as.integer(format(dat$ti_var, "%Y")) + 
    (as.integer(format(dat$ti_var, "%m")) - 1L)/12L
  gam_fit <- bam(
    y ~ ti(ti_var, k = 15, bs = "cr") + 
      ti(wz_rel_size, k = 15, bs = "cr") +
      ti(ti_var, wz_rel_size, k = c(15, 15), bs = "cr"), 
    binomial("cloglog"), dat, cluster = cl)
  stopCluster(cl)
  
  print(summary(gam_fit))
      
  get_plot_device({
      plot(gam_fit)
      n_plots <- 9L
      for(i in 0:(n_plots - 1L))
        vis.gam(gam_fit, ticktype = "detailed", theta = -45 + 360 * i / n_plots, 
                color = "gray", zlab = "Log-hazard", xlab = "Year", 
                ylab = "Log relative market size")
    }, "size-time-marg", onefile = FALSE)
})
```

</div>

Get approximate negative observed information matrix

<div class="hideable"> 

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "sd_w_size_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r sd_w_size_rng, cache = 1}
state_rng_size_derivs <- 
  get_approx_info_n_score(state_rng_size_restrict_large)
```

```{r show_sd_w_size_rng}
local({
  idx <- c(1, 4:8)
  print(x <- solve(state_rng_size_derivs$state$neg_obs_info[idx, idx]))
  print(sqrt(diag(x)))
})

sqrt(diag(solve(state_rng_size_derivs$observation$neg_obs_info)))
```

</div>

### Model with a random relative size slope -- unrestricted

<div class="hideable">

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "unrestricted_w_size_state_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r unrestricted_w_size_state_rng, cache = 1, dependson = "dd_fixed"}
set.seed(seed)
log_n_eval({
  state_rng_size_unrestrict <- PF_EM(
    fixed = frm_fixed, random = ~ 1 + wz(rel_size),  
    data = dat, model = "cloglog", by = 1L, max_T = max(dat$tstop), 
    id = dat$gvkey_unique, type = "VAR",  Q_0 = diag(c(1, 2)^2),
    Q = matrix(c(.12, .02, .02, .06), 2), Fmat = diag(.8, 2),
    control = ctrl_default, 
    trace = 1L)
}, "_state_rng_size_unrestrict")
```

Take more iterations

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "xtra_unrestricted_w_size_state_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r xtra_unrestricted_w_size_state_rng, cache = 1, dependson = "unrestricted_w_size_state_rng"}
set.seed(seed)   
log_n_eval({
  state_rng_size_unrestrict_xtra <- take_xtra(
    state_rng_size_unrestrict, trace = 1L)
}, "_state_rng_size_unrestrict_xtra")
```

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "large_unrestricted_w_size_state_rng", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r large_unrestricted_w_size_state_rng, cache = 1, dependson = "xtra_unrestricted_w_size_state_rng"}
state_rng_size_unrestrict_large <- take_xtra(
  state_rng_size_unrestrict_xtra, n_max = 1L, N_fw_n_bw = 2000L, 
  N_smooth = 10000L, N_smooth_final = 2500L, N_first = 2500L)
```

</div>

Check results

<div class="hideable">

```{r unrestricted_show_xtra_w_size_state_rng}
#####
# approx log-likelihood
local({
  n <- length(state_rng_size_unrestrict$log_likes) + 
    length(state_rng_size_unrestrict_xtra$log_likes)
  with(state_rng_size_unrestrict, {
    plot(1:length(log_likes), log_likes, xlim = c(1, n), 
         ylim = range(state_rng_size_unrestrict_xtra$log_likes, log_likes))  
  })
  with(state_rng_size_unrestrict_xtra, {
    points((n - length(log_likes) + 1L):n, log_likes, xlim = c(1, n), pch = 16)  
  })
})

plot(state_rng_size_unrestrict_xtra$log_likes)
logLik(state_rng_size_unrestrict_large)

#####
# estimates
state_rng_size_unrestrict_xtra$EM_ests
sqrt(diag(state_rng_size_unrestrict_large$Q))
cov2cor(state_rng_size_unrestrict_large$Q)
state_rng_size_unrestrict_large$F

# compare  with 
sqrt(diag(state_rng_size_restrict_large$Q))
cov2cor(state_rng_size_restrict_large$Q)
state_rng_size_restrict_large$F

get_plot_device(
  pf_plot_effects(state_rng_size_unrestrict_large, 
                  ylabs = c("Intercept", get_label("rel_size"))), 
  "rng-inter-smooth-state-w-size-unrestrict", onefile = FALSE)

compare_fix(state_rng_size_unrestrict_large, "comp-fix-inter-rel-size-unrestrict")

plot_sp_w_c(
  "sigma"   , state_rng_size_unrestrict_large, xlab = get_label("sigma"))
plot_sp_w_c(
  "r_niq_atq", state_rng_size_unrestrict_large, xlab = get_label("r_niq_atq"))
plot_sp_w_c(
  "r_mv_ltq_log", state_rng_size_unrestrict_large, xlab = 
    get_label("r_mv_ltq_log"))

#####
# effective sample size and AIC
plot_eff(state_rng_size_unrestrict)
plot_eff(state_rng_size_unrestrict_xtra)
plot_eff(state_rng_size_unrestrict_large)

AIC(base_fit, duffie_09_xtra, state_rng_inter_large, 
    state_rng_size_restrict_large, state_rng_size_unrestrict_large)
```

</div>

## Create estimates table

<div class="hideable">

```{r create_coef_table}
local({
  #####
  # get the input we need
  stopifnot(isTRUE(all.equal(
    names(state_rng_inter_large$fixed_effects), 
    names(state_rng_size_restrict_large$fixed_effects))))
  betas <- cbind(state_rng_inter_large$fixed_effects, 
                 state_rng_size_restrict_large$fixed_effects)
  
  labs <- attr(state_rng_inter_large$terms$fixed, "term.labels")
  labs <- c("(Intercept)", labs)
  
  vcovs <- list(
    solve(state_rng_inter_derivs$observation$neg_obs_info),
    solve(state_rng_size_derivs$observation$neg_obs_info))
  
  # re-order terms
  assg <- 
    attr(model.matrix(state_rng_inter_large$terms$fixed, dat), "assign") + 1L
  
  frm_ord <- # we will use this order
    ~ wz(dtd) + wz(excess_ret) + r1y + log_market_ret + 
    wz(actq_defl_log) + wz(r_wcapq_atq) + wz(r_oiadpq_atq) + 
    wz(r_mv_ltq_log) + wz(r_niq_atq) + wz(r_ltq_atq) + wz(r_actq_lctq) + 
    wz(r_actq_lctq) + wz(sigma) + wz(rel_size) + 
    sp_w_c(r_niq_atq, 3) + sp_w_c(sigma, 3) + 
    sp_w_c(r_mv_ltq_log, 3) + wz(r_actq_lctq):wz(sigma)
  o <- c("(Intercept)", attr(terms(model.frame(frm_ord, dat)), "term.labels"))
  stopifnot(setequal(o, labs))
  
  df <- unlist(sapply(assg, function(x) sum(assg == x)))
  
  i_new <- match(o, labs)
  i_new <- unlist(sapply(i_new, function(x) which(assg == x)))
  betas <- betas[i_new, ]
  assg <- assg[i_new]
  df <- df[i_new]
  vcovs <- lapply(vcovs, "[", i = i_new, j = i_new)
  labs <- o
  
  # compute p-values
  pvs <- lapply(unique(assg), function(a){
    idx <- which(assg == a)
    df_term <- length(idx)
    x  <- betas[idx, , drop = FALSE]
    cv <- lapply(vcovs, "[", i = idx, j = idx, drop = FALSE)
    chi_vals <- mapply(
      function(cvmat, b) drop(b %*% solve(cvmat, b)), 
      cvmat = cv, b = split(x, rep(1:ncol(x), each = nrow(x))))
    cbind(stat = chi_vals, df = df_term, 
          p = pchisq(chi_vals, df = df_term, lower.tail = FALSE))
  })
  names(pvs) <- labs
  stats <- t(sapply(pvs, "[", i = TRUE, j = "stat"))
  pvals <- t(sapply(pvs, "[", i = TRUE, j = "p"))
  
  do_drop <- duplicated(assg)
  betas <- betas[!do_drop, ]
  df <- df[!do_drop]
  rownames(betas) <- labs
  
  # blank those out with more than one term
  betas[df > 1L, ] <- NA_real_
  
  theta <- matrix(NA_real_, 2, 2)
  theta[1, 1] <- state_rng_inter_large$F
  theta[, 2]  <- diag(state_rng_size_restrict_large$F)
  
  sds <- matrix(NA_real_, 2, 2)
  sds[1, 1] <- sqrt(state_rng_inter_large$Q)
  sds[, 2]  <- sqrt(diag(state_rng_size_restrict_large$Q))
  
  cors <- matrix(NA_real_, 1, 2)
  cors[, 2] <- cov2cor(state_rng_size_restrict_large$Q)[1, 2]
  
  rest <- sapply(list(state_rng_inter_large, state_rng_size_restrict_large), 
                 function(x) c(AIC(x), logLik(x), length(unique(dat$gvkey))))
  row.names(rest) <- c("AIC", "log-likelihood", "# firms")
  
  #####
  # make table
  t_arg <- paste(
    "S[table-format=-2.3 ,table-alignment=right]@{}", 
    "@{}l", 
    "S[table-format=2.3,table-space-text-pre={*}, table-space-text-post={-*}]", 
    sep = "\n")
  cmark <- "{\\makecell[r]{\\checkmark}}"
  
  nm <- 2L
  cat(
    "\\begin{tabular}{l\n", paste0(rep(t_arg, nm), collapse = "\n"), "}\n",
    sep = "", "\\toprule\n", 
    "& ", paste0("\\multicolumn{3}{c}{$\\mathcal{M}_", 
                 1:nm + 3L, "$}", collapse = " & "), "\\\\\n", 
    "\\midrule\n")
  
  # fixed effect coefficients
  get_pstart <- function(x, p){
    s <- ifelse(
      p > .1 | is.na(p), "", ifelse(
        p > .05, "$^{*}$", ifelse(
          p > .01, "$^{**}$", "$^{***}$")))
    
    ifelse(is.na(x), " & ", paste0(x, " & ",  s))
  }
  for(i in 1:nrow(betas)){
    # row name
    na <- rownames(betas)[i]
    na <- if(na == "(Intercept)") "Intercept" else {
      na <- gsub(
        "(wz\\()([a-zA-Z0-9_]+)(,.+|\\))", "\\2", na, perl = TRUE)
      regexp <- "(sp_w_c\\()([a-zA-Z0-9_]+)(,.+|\\))"
      is_spline <- grepl(regexp, na, perl = TRUE)
      na <- gsub(regexp, "\\2", na, perl = TRUE)
      
      if(is_spline) 
        paste0(get_label(na), " (spline)") else 
          get_label(na)
    }
    na <- gsub("\\*", "$\\\\cdot$", na)
    
    cat(na, "& ")
    z <- betas[i, ]
    z <- ifelse(is.na(z), cmark, sprintf("%.3f", z))
    z <- get_pstart(z, pvals[i, ])
    test_stats <- sprintf("(%.1f)", stats[i, ])
    cat(rbind(z, test_stats), sep = " & ")
    cat(" \\\\\n")
  }
  cat("\\midrule\n")
  
  # parameters related to frailty model
  fra_params <- rbind(theta, sds, cors)
  stopifnot(nrow(cors) == 1L)
  rownames(fra_params) <- c(
    paste0("$\\theta_{", 1:nrow(theta), "}$"),
    paste0("$\\sigma_{", 1:nrow(sds), "}$"), 
    "$\\rho$")
  
  for(i in 1:nrow(fra_params)){
    z <- fra_params[i, ]
    cat(rownames(fra_params)[i], "&", paste0(
      ifelse(is.na(z), "", sprintf("%.3f", z)), collapse = " & & & "), 
      "\\\\\n")
  }
  cat("\\midrule\n")
  
  # summary stats
  cat("AIC", "&", paste0(
    sprintf("%.1f", rest["AIC", ]) , collapse = " & & & "), "\\\\\n")
  cat("log-likelihood", "&", paste0(
    sprintf("%.1f", rest["log-likelihood", ]) , collapse = " & & & "), 
    "\\\\\n")
  cat("Number of firms", "&", paste0(
    sprintf("%d", rest["# firms", ]) , collapse = " & & & "), "\\\\\n")
  
  cat("\\bottomrule\n\\end{tabular}")
})
```

</div>

## Plot splines

<div class="hideable">

```{r plot_splines}
get_plot_device({
  x1 <- plot_sp_w_c("sigma", state_rng_size_restrict_large, vals_only = TRUE)
  x2 <- plot_sp_w_c("sigma", state_rng_inter_large, vals_only = TRUE)
  matplot(
    x1$x, cbind(x1$base, x1$rng, x2$rng, x1$base_lb, x1$base_ub), 
    lty = c(3, 1, 2, 3, 3), col = "black", 
    type = "l", xlab = get_label("sigma"), ylab = "Log-hazard term")
  
  x1 <- plot_sp_w_c("r_niq_atq", state_rng_size_restrict_large, vals_only = TRUE)
  x2 <- plot_sp_w_c("r_niq_atq", state_rng_inter_large, vals_only = TRUE)
  matplot(
    x1$x, cbind(x1$base, x1$rng, x2$rng, x1$base_lb, x1$base_ub), 
    lty = c(3, 1, 2, 3, 3), col = "black", 
    type = "l", xlab = get_label("r_niq_atq"), ylab = "Log-hazard term")
  
  x1 <- plot_sp_w_c("r_mv_ltq_log", state_rng_size_restrict_large, vals_only = TRUE)
  x2 <- plot_sp_w_c("r_mv_ltq_log", state_rng_inter_large, vals_only = TRUE)
  matplot(
    x1$x, cbind(x1$base, x1$rng, x2$rng, x1$base_lb, x1$base_ub), 
    lty = c(3, 1, 2, 3, 3), col = "black", 
    type = "l", xlab = get_label("r_mv_ltq_log"), ylab = "Log-hazard term")
}, "splines-rng-monthly", onefile = FALSE)
```

</div>

## In-sample

Define function to make in-sample simulation

<div class="hideable">

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "in_sample_sim", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
-->

```{r in_sample_sim, cache=1}
sim_func <- function(
  object, n_sim = 10000L, data = dat, id = gvkey_unique, tvar = tstop, 
  tstop = min(dat$tstop)){
  stopifnot(any(inherits(object, c("PF_EM", "glm")) > 0))
  is_pf_em <- inherits(object, "PF_EM")
  
  # get data
  id   <- eval(substitute(id)  , data)
  tvar <- eval(substitute(tvar), data)
  
  if(is_pf_em){
    # find clouds
    cl_start <- min(tvar) - tstop + 1L
    cl <- object$clouds$smoothed_clouds
    if(cl_start > 1L)
      cl <- cl[-seq_len(cl_start - 1L)]
    
  }
  
  # set indicies 
  tvar_shift <- min(tvar) - 1L
  tvar_idx <- tvar - tvar_shift
  
  # assign covariates and random effect design matrix
  eta_fix <- 
    drop(if(is_pf_em) 
      model.matrix(object$terms$fixed, data) %*% object$fixed_effects else
        model.matrix(object$terms, data) %*% coef(object))
  if(is_pf_em) 
    Z <- model.matrix(object$terms$random, data)
  
  t_points <- unique(tvar_idx)
  o <- lapply(t_points, function(idx){
    # get subset of data that we need
    keep <- which(tvar_idx == idx)
    data_t  <- data   [keep, ]
    eta_fix <- eta_fix[keep]
    id      <- id[keep]
    if(is_pf_em){
      Z_t     <- Z      [keep, , drop = FALSE]
      cl_t    <- cl     [[idx]] 
      ws <- cl_t$weights
      states <- cl_t$states
    }
    
    # find average default rate
    y_dist <- replicate(n_sim, {
      if(is_pf_em){
        # draw particle
        p_idx <- sample(length(ws), prob = ws, size = 1L) 
        part <- states[, p_idx, drop = FALSE]
        eta <- drop(eta_fix + Z_t %*% part)
        
      } else 
        eta <- eta_fix
      
      outcome <- -expm1(-exp(eta)) > runif(length(eta))
      mean(outcome)
    })
    
    eta_means <- if(is_pf_em) 
      # compute mean log odds for each firm
      colSums(t(Z_t %*% states) * drop(ws)) + eta_fix else
        eta_fix
    
    tvar <- idx + tvar_shift
    list(
      y_dist = y_dist, tvar = tvar, 
      pred = data.frame(id = id, tvar = tvar, eta_mean = eta_means))
  })
  
  # gather results and return
  pred <- do.call(rbind, lapply(o, "[[", "pred"))
  y_dists <- sapply(o, "[[", "y_dist")
  colnames(y_dists) <- (y_dist_nam <- sapply(o, "[[", "tvar"))
  y_dists <- y_dists[, order(y_dist_nam)]
  
  list(pred = pred, y_dists = y_dists)
}

# fit other models to compare with
duf_fit <- glm(y ~ wz(dtd) + wz(excess_ret) + r1y + log_market_ret, 
               binomial("cloglog"), dat)
rng_fix <- update(
  base_fit, 
  . ~ . + 
    (1 + wz(rel_size))*ns(tstart, intercept = FALSE, df = 5))

dat$year  <- as.integer(format(make_ym_inv(dat$tstop), "%Y"))
dt_sub <- subset(dat, year >= 2000L)
set.seed(99969118)
duf_base  <- sim_func(duf_fit , data = dt_sub)
out_base  <- sim_func(base_fit, data = dt_sub)
fix_base  <- sim_func(rng_fix, data = dt_sub)
out_inter <- sim_func(state_rng_inter_large, data = dt_sub)
out_size  <- sim_func(state_rng_size_restrict_large, data = dt_sub)

rm(sim_func)
```

Check results

```{r in_sample_sim_illu}
#####
# look at AUC
objs <- list(
  duf = duf_base, base = out_base, inter = out_inter, size = out_size, 
  rng_fix = fix_base)
rocs <- lapply(objs, function(out){
  library(pROC)
  tmp <- merge(dat[, c("y", "gvkey_unique", "tstart")], out$pred, 
               by.x = c("gvkey_unique", "tstart"), by.y = c("id", "tvar"))
  roc <- with(tmp, roc(y, eta_mean))
  list(roc = roc, partial = auc(roc, partial.auc = c(1, .9)))  
})

print(rocs$duf    , digits = 4)
print(rocs$base   , digits = 4)
print(rocs$rng_fix, digits = 4)
print(rocs$inter  , digits = 4)
print(rocs$size   , digits = 4)

plot(rocs$duf    $roc, col = "darkgray", grid = TRUE)
plot(rocs$base   $roc, col = "black", add = TRUE)
plot(rocs$inter  $roc, col = "brown", add = TRUE)
plot(rocs$inter  $roc, col = "darkblue", add = TRUE)
plot(rocs$rng_fix$roc, col = "darkblue", lty = 2, add = TRUE)

#####
# look at default rate distribution
rates <- lapply(
  objs, function(out)
  apply(out$y_dists, 2, quantile, probs = c(.05, .5, .95)))

func <- function(rates = rates, idx){
  # get name of the index, lower and upper bounds, and the median
  idx <- deparse(substitute(idx))
  rts <- rates[[idx]]
  lbs <- rts[1, ]
  med <- rts[2, ]
  ubs <- rts[3, ]
  
  # compute realized rates and add color for coverage
  dt_sub <- subset(dat, tstop %in% colnames(rts), c(y, tstop))
  y_rea <- tapply(dt_sub$y, dt_sub$tstop, mean)
  is_cov <- y_rea >= lbs & y_rea <= ubs
  cat(sprintf("Coverage is %.2f pct.\n", mean(is_cov * 100)))
  col <- ifelse(is_cov, "DarkGray", "Black")
  
  # make plot w/ medians
  x_vars <- make_ym_inv(as.integer(colnames(rts)))
  plot(x_vars, med, ylim = range(rts), xlab = "Year",  
       ylab = paste("Default rate w/", sQuote(idx)), col = col)
  
  # add realized 
  points(x_vars, y_rea, col = col, pch = 4)
  
  # add prediction interval
  arrows(x_vars, lbs, x_vars, ubs, length = 0.05, angle = 90, code = 3, 
         col = col)
}

func(rates, duf)
func(rates, base)
func(rates, inter)
func(rates, size)
func(rates, rng_fix)
```

Check those with the largest difference

```{r check_w_large_diff}
local({
  eta_diff <- out_size$pred$eta_mean - out_base$pred$eta_mean
  ord <- order(eta_diff)
  ord <- c(head(ord, 500), tail(ord, 500))
  
  # is it time specific?
  print(lattice::stripplot(
    out_size$pred$tvar[ord], col = "black", method="jitter", jitter = 1))
  plot(state_rng_size_restrict_large, cov_index = 1)
  delta_ts <- c(500, 550, 650) - min(dat$tstart) + 1L
  sapply(delta_ts, function(x) abline(v = x, lty = 2))
  
  set.seed(99969118)
  ord <- sample(ord, 10L, replace = FALSE)
  
  to_check <- out_size$pred[ord, ]

  vs <- all.vars(update(frm_fixed, 1L ~ .))
  dt <- dat
  dt[colnames(dt) %in% vs] <- lapply(dt[colnames(dt) %in% vs], 
                                     function(x) scale(wz(x)))
  
  z <- merge(dt, to_check, by.x = c("gvkey_unique", "tstop"), 
             by.y = c("id", "tvar"))
  
  z <- cbind(`eta diff` = eta_diff[ord], z[, c("y", "tstop", vs)], 
             `eta base` = out_base$pred$eta_mean[ord], 
             `eta rng`  = out_size$pred$eta_mean[ord])
  z <- z[order(z$`eta diff`), ]
  flip <- sapply(z, function(x) is.numeric(x) && !is.integer(x))
  z[flip] <- lapply(z[flip], signif, digits = 2)
  
  cat("Ordered by difference in linear predictor\n")
  print(z)
  
  cat("Ordered by difference time\n")
  z[order(z$tstop, z$`eta diff`), ]
})
```

</div>

## Out-of-sample

Clean-up

<div class="hideable">

```{r pre_out_sample_clean_up}
eval(local({
  to_rm <- c("rng_fix", "rocs", "objs", ls(pattern = "state_rng_", pos = 1), 
             ls(pattern = "duffie_09", pos = 1), "duf_fit")
  cat("Removing:\n",  paste0(sQuote(to_rm), collapse = " "), "\n")
  to_rm <- c(list(quote(rm)), as.list(to_rm))
  as.call(to_rm)
}))
```

</div>

Fit models for out-of-sample test

<div class="hideable">

```{r fit_out_of_sample}
# make dummy with half year indicator
dat$yh <- local({
  start_date     <- make_ym_inv(dat$tstart)
  y <- as.integer(format(start_date, "%Y"))
  m <- as.integer(format(start_date, "%m"))
  
  yh <- paste0("Y", y, "H", ifelse(m < 7, 1L, 2L))
  
  # print examples
  set.seed(11640787)
  print(data.frame(start_date, yh)[sample.int(length(y), size = 30L), ])
  
  as.factor(yh)
})

# fit models
ctrl_default_out_of_sample <- PF_control(
  N_fw_n_bw = 1000L, N_smooth = 1000L, N_first = 2000L, n_max = 1000, 
  n_threads = n_threads, nu = 8L, covar_fac = 1.2, ftol_rel = 1e-6,
  smoother = "Fearnhead_O_N", eps = 1e-4, averaging_start = 50L)

do_re_run_expr <- expression(
  if(exists("do_re_run"))
    do_re_run else 
      do_re_run <- as.character(tcltk::tkmessageBox(
        message = "Do you want to re-run the out-of-sample models?", 
        type = "yesno")) == "yes")

f1_prefix <- file.path(
  "markdown", "output", "outsample-state-rng-inter-monthly")
f2_prefix <- file.path(
  "markdown", "output", "outsample-state-rng-size-inter-monthly")

to_fit <- local({
  o <- sort(unique(dat$yh))
  o[grepl("199[89]|200\\d|201[0-5]", as.character(o))]
})
to_fit

for(i in seq_along(to_fit)){
  y_i <- to_fit[i]
  dat_y <- subset(dat, as.integer(yh) <= as.integer(y_i))
  cat("Fitting models with these periods\n")
  print(sort(unique(dat_y$yh)))
  
  #####
  # fit model with random intercept
  f_name <- paste0(f1_prefix, "-", y_i, ".RDS")
  if(!file.exists(f_name) || eval(do_re_run_expr)){
    set.seed(54820338)
    log_n_eval({
      fit <- PF_EM(
        fixed = frm_fixed, random = ~ 1, 
        data = dat_y,
        model = "cloglog", by = 1L, max_T = max(dat_y$tstop), 
        id = dat_y$gvkey_unique, 
        
        # start close to values from full period as of 2019/04/04
        Q = diag(0.0101, 1), Fmat = diag(0.912, 1), 
        fixed_effects = c(
          -10.5, 0.205, -0.981, -1.08, -2.02, 0.447, -0.845, 109, -1.73, 
          -0.34, 0.138, 0.938, 5.6, -0.172, 0.648, -4.19, -1.61, -0.733, 
          -1.84, 0.39, 16.1),
        
        type = "VAR", control = ctrl_default_out_of_sample, 
        trace = 1L)
    }, paste0("_outsample-rng-inter-monthly-", y_i))
    
    saveRDS(fit, f_name)
  }
  
  #####
  # fit model with random intercept and size slope
  f_name <- paste0(f2_prefix, "-", y_i, ".RDS")
  if(!file.exists(f_name) || eval(do_re_run_expr)){
    set.seed(54820338)
    log_n_eval({
      fit <- PF_EM(
        fixed = frm_fixed, random = ~ 1 + wz(rel_size), 
        data = dat_y,
        model = "cloglog", by = 1L, max_T = max(dat_y$tstop), 
        id = dat_y$gvkey_unique, 
        
        J = diag(1, 2), Q_0 = diag(1, 2),
        G = matrix(c(1, 0, 
                     0, 0, 
                     0, 0, 
                     0, 1), byrow = TRUE, ncol = 2),  
        K = matrix(1, 1, 1),
        
        # start close to values from full period as of 2019/03/12
        theta = c(0.971, 0.974), phi = 4.74, psi = c(-1.36, -2.82), 
        fixed_effects = c(
          -10.8, 0.252, -1.1, -1.09, -1.93, 0.521, -0.813, 126, -1.67, 
          -0.42, 0.106, 0.972, -0.381, -0.138, 0.63, -5.05, -1.92, -0.627, 
          -1.75, 0.507, 15.2),
        
        type = "VAR", control = ctrl_default_out_of_sample, 
        trace = 1L)
    }, paste0("_outsample-rng-size-monthly-inter-", y_i))
    
    saveRDS(fit, f_name)
  }
}
```

</div>

Load and check result

<div class="hideable">

```{r check_fit_out_of_sample}
# find the files
fs <- list.files(file.path("markdown", "output"), full.names = TRUE)
inter_ms <- fs[
  grepl("outsample\\-state\\-rng\\-inter\\-monthly\\-Y", fs)]
size_inter_ms <- fs[
  grepl("outsample\\-state\\-rng\\-size\\-inter\\-monthly\\-Y", fs)]

# add the years
names(inter_ms) <- gsub("(.+)(Y\\d{4}H\\d)(\\.RDS)$", "\\2", inter_ms)
names(size_inter_ms) <- gsub("(.+)(Y\\d{4}H\\d)(\\.RDS)$", "\\2", size_inter_ms)

# load the objects
inter_ms <- lapply(inter_ms, readRDS)
size_inter_ms <- lapply(size_inter_ms, readRDS)

# check that they all converged
par(mar = c(5, 4, .5, .5), mfcol = c(2, 2))
for(o in names(inter_ms))
  plot(inter_ms[[o]]$log_likes, ylab = o, type = "l")
for(o in names(size_inter_ms))
  plot(size_inter_ms[[o]]$log_likes, ylab = o, type = "l")

# quickly check coefficients
x <- signif(sapply(inter_ms, "[[", "fixed_effects"), 4)
x["(Intercept)", ]
# scale by standard deviations
sds <- apply(model.matrix(tail(inter_ms)[[1L]]$terms$fixed, dat), 2, sd)
sds["(Intercept)"] <- NA
round(x <- t(x * sds)           , 1)
round(z <- t(t(x) - colMeans(x)), 1)
par(mar = c(5, 4, 1, 1), mfcol = c(1, 1))
matplot(z, type = "l", lty = 1)

sqrt(sapply(inter_ms, "[[", "Q"))
sapply(inter_ms, "[[", "F")

x <- signif(sapply(size_inter_ms, "[[", "fixed_effects"), 4)
x["(Intercept)", ]
# scale by standard deviations
sds <- apply(
  model.matrix(tail(size_inter_ms)[[1L]]$terms$fixed, dat), 2, sd)
sds["(Intercept)"] <- NA
round(x <- t(x * sds)           , 1)
round(z <- t(t(x) - colMeans(x)), 1)
matplot(z, type = "l", lty = 1)

sapply(size_inter_ms, "[[", "Q")
sqrt(sapply(lapply(size_inter_ms, "[[", "Q"), diag))
sapply(size_inter_ms, "[[", "theta")

ll1 <- sapply(inter_ms     , logLik)
ll2 <- sapply(size_inter_ms, logLik)
par(mar = c(5, 4, .5, .5), mfcol = c(1, 1))
plot(ll2 - ll1, ylim = range(0, ll2 - ll1))

# plot smoothed particles
par(mar = c(5, 4, .5, .5), mfcol = c(2, 2))
local({
  min_x  <- min(as.integer(dat$tstart))
  min_yr <- as.integer(format(make_ym_inv(min_x), "%Y"))
  x_lim  <- range(make_ym_inv(dat$tstart))
  
  get_x_max <- function(x)
    with(dat, max(tstart[yh == o]))
  
  for(o in names(inter_ms)){
    max_x <- get_x_max(o)
    x <- make_ym_inv(min_x:max_x)
    pf_plot_effects(
      inter_ms[[as.character(o)]], x = x, ylabs = "Intercept", 
      xlim = x_lim, change_par = FALSE)
  }
  
  for(o in names(size_inter_ms)){
    max_x <- get_x_max(o)
    x <- make_ym_inv(min_x:max_x)
    pf_plot_effects(
      size_inter_ms[[as.character(o)]], x = x, xlim = x_lim,
      ylabs = c("Intercept", get_label("rel_size")), change_par = FALSE)
  }
})

# plot effective sample size of forward particle filter
par(mar = c(5, 4, .5, .5), mfcol = c(2, 2))
local({
  for(o in inter_ms)
    plot(o$effective_sample_size$forward_clouds, type = "h", 
         ylim = range(0, o$effective_sample_size$forward_clouds))
  
  for(o in size_inter_ms)
    plot(o$effective_sample_size$forward_clouds, type = "h", 
         ylim = range(0, o$effective_sample_size$forward_clouds))
})
```

</div>

Forecast outcomes with models with frailty

<div class="hideable">

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "forecast", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
--> 

```{r forecast, cache = 1}
# assign function to perform the simulation
forecast_func <- function(
  fit, yh_i, n_outer = 10000L, n_inner = 10L, cl, idvar = gvkey, tvar = tstop){
  stopifnot(is.factor(y_i), is.integer(n_outer), is.integer(n_inner), 
            n_outer > 0L, n_inner > 0L, inherits(fit, c("PF_EM", "glm")))
  
  is_glm <- inherits(fit, "glm")
  if(is_glm)
    stopifnot(
      fit$family$family == "binomial", fit$family$link == "cloglog") else
      stopifnot(fit$call$model == "cloglog")
  
  # get data. `yh_i` is the period up to which we use to estimate the model 
  # so we increament by one
  yh_i <- as.integer(yh_i) + 1L
  dat_y <- subset(dat, as.integer(yh) == yh_i)
  
  # get id and time variable
  id <- eval(substitute(idvar), dat_y)
  ti <- eval(substitute(tvar) , dat_y)
  ti <- ti - min(ti) + 1L
  # check that data is sorted
  stopifnot(!is.unsorted(id), all(!tapply(ti, id, is.unsorted)))
  
  ti_unique <- unique(ti)
  # time points should be consecutive
  stopifnot(all(diff(ti_unique) == 1L))
  
  # get design matrices
  if(is_glm){
    phat <- predict(fit, dat_y, type = "response")
    clusterExport(
      cl, c("phat", "id", "ti", "ti_unique", "n_inner", "is_glm"), 
      environment())
    
  } else {
    eta <- drop(model.matrix(fit$terms$fixed, dat_y) %*% fit$fixed_effects)
    Z <- model.matrix(fit$terms$random, dat_y)
    cloud <- tail(fit$clouds$forward_clouds, 1)[[1L]]
    F. <- fit$F
    Q_chol <- t(chol(fit$Q))
  
    clusterExport(
      cl, c("eta", "Z", "cloud", "F.", "Q_chol", "id", "ti", "ti_unique", 
            "n_inner"), environment())
    
  }
  
  # simulate states, compute probabilities, and simulate outcomes
  out <- parLapply(cl, 1:n_outer, function(...){
    if(!is_glm){
      # draw parent
      idx <- sample.int(length(cloud$weights), 1L, prob = cloud$weights)
      parent <- cloud$states[, idx, drop = FALSE]
      
      # simulate path 
      q <- nrow(parent)
      n <- length(ti_unique)
      path <- replicate(n, Q_chol %*% rnorm(q), simplify = FALSE)
      path <- Reduce(
        function(x, y) y + F. %*% x, path, init = parent, accumulate = TRUE)
      path <- if(!is.list(path))
        matrix(path, 1L) else do.call(cbind, path)
      
      # compute default probabilities, simulate outcomes, and simulate industry 
      # wide default rate
      log_haz <- rep(NA_real_, length(eta))
      for(t. in ti_unique){
        idx <- which(ti == t.)
        log_haz[idx] <- Z[idx, , drop = FALSE] %*% path[, t.]
      }
      stopifnot(!anyNA(log_haz))
      log_haz <- log_haz + eta
     
      phat <- -expm1(-exp(log_haz)) 
    }
    
    # compute probability of default over the whole period. It ignores 
    # missing intermediary values due to missing covariates and potentially
    # recurrent events within the same year
    phat <- tapply(
      phat, id, Reduce, f = function(x, y) x + (1 - x) * y, init = 0.)
    id_out <- names(phat)
    
    Y <- replicate(n_inner, phat > runif(length(phat)))
    Y_bar <- colMeans(Y)
    
    list(phat = if(is_glm) NULL else phat, id_out = id_out, Y_bar = Y_bar)
  })
  
  # gather default rates for individuals, compute the means, and gather the 
  # industry wide default rates
  phat <- if(!is_glm)
    rowMeans(sapply(out, "[[", "phat")) else 
      tapply(phat, id, Reduce, f = function(x, y) x + (1 - x) * y, init = 0.)
  id_out <- out[[1L]]$id_out
  Y_bar <- c(sapply(out, "[[", "Y_bar"))
  
  # compute realized event counter for individuals
  event_count <- tapply(dat_y$y, id, sum)
  stopifnot(# id indices match 
    all(names(event_count) == id_out))
  
  # how many observations do we have after an event and how many firms have 
  # more than one event? We pose these questions as we assume that an 
  # event is an absorbing state in the short term in the above code
  cum_event_count <- sapply(
    tapply(dat_y$y, id, cumsum), function(x) x[-length(x)] > 0L)
  cat(sprintf(
    "%.2f pct. observations are after an event and we have %3d firms with more than one event\n", 
    mean(unlist(cum_event_count)) * 100, sum(event_count > 1L)))
  
  list(
    Y_bar = Y_bar, phat = phat, id_out = id_out, event_count = event_count)
}

# use function
library(parallel)
cl <- makeCluster(6L)
set.seed(64010384)
clusterSetRNGStream(cl)

inter_ms_forceast <- mapply(
  forecast_func, fit = inter_ms, SIMPLIFY = FALSE, MoreArgs = list(cl = cl),
  yh_i = factor(names(inter_ms), levels = levels(dat$yh)))
size_inter_ms_forceast <- mapply(
  forecast_func, fit = size_inter_ms, SIMPLIFY = FALSE, 
  MoreArgs = list(cl = cl), 
  yh_i = factor(names(inter_ms), levels = levels(dat$yh)))

stopCluster(cl)
```

</div>

Do the same for models without frailty

<div class="hideable">

<!--
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "out_sample_no_frailty", 
      path = paste0(file.path("markdown", "cache", "rng-effects"), 
                    .Platform$file.sep))
-->

```{r out_sample_no_frailty, cache = 1}
# define function to fit models and use previous defined function to simulate
# the predicted default rate and more
out_sample_glm <- function(frm, cl, n_outer = 10000L, n_inner = 10L){
  to_fit <- local({
    o <- sort(unique(dat$yh))
    o[grepl("199[89]|200\\d|201[0-5]", as.character(o))]
  })
  
  out <- lapply(to_fit, function(yh_i){
    # fit model
    fit <- glm(
      frm, family = binomial("cloglog"), 
      data = subset(dat, as.integer(yh) <= as.integer(yh_i)))
    
    # forecast
    out <- forecast_func(
      fit = fit, yh_i = yh_i, n_outer = n_outer, n_inner = n_inner, cl = cl)
    c(out, list(coef = coef(fit)))
  })
  
  names(out) <- as.character(to_fit)
  out
}

# use function
library(parallel)
cl <- makeCluster(6L)
set.seed(64010384)
clusterSetRNGStream(cl)

frm_simple <- y ~ wz(dtd) +  wz(excess_ret) + log_market_ret + r1y
duffie_et_al <- out_sample_glm(frm_simple, cl)
base_fit_outsample <- out_sample_glm(frm, cl)

stopCluster(cl) 
rm(out_sample_glm)
```

Check results

```{r check_out_sample_no_frailty}
# coefficients
signif(x <- do.call(cbind, lapply(duffie_et_al, "[[", "coef")), 2)
signif(x <- do.call(cbind, lapply(base_fit_outsample, "[[", "coef")), 2)
```

</div>

Illustrate results

<div class="hideable">

```{r out_of_sample_illustrations}
#####
# Make plots and figures. Start by getting figures
objs <- list(
  `Duffie et al.`          = duffie_et_al, 
  `No frailty`             = base_fit_outsample,
  `Frailty: inter`         = inter_ms_forceast, 
  `Frailty: inter & slope` = size_inter_ms_forceast) 
stopifnot(all(sapply(objs[-1L], length) == length(objs[[1L]])), 
          all(sapply(objs[-1L], names) == names(objs[[1L]])))

fitted_lvls <- local({
  fs <- names(objs[[1L]])
  unique(dat$yh[dat$yh %in% fs])
})
forecast_summary <- lapply(seq_along(fitted_lvls), function(i){
  # check that indices match
  yh_i <- fitted_lvls[i]
  objs_i <- lapply(objs, "[[", i)
  indices <- lapply(objs_i, "[[", "id_out")
  stopifnot(all(
    sapply(indices[-1], function(z) all(indices[[1L]] == z))))
  
  # compute AUCs
  y_realized <- objs_i[[1L]]$event_count > 0L
  library(pROC)
  aucs <- if(!any(y_realized) || !any(!y_realized)){
    cat("Cannot compute auc for", sQuote(yh_i), "\n")
    aucs <- rep(NA_real_, length(objs_i))
    
  } else 
    sapply(objs_i, function(x) c(auc(y_realized, x$phat)))
  
  # computed predicted industry default rate quantiles
  default_rate_dist <- sapply(objs_i, function(x) 
    quantile(x$Y_bar, c(.05, .5, .95)))
  
  # compute realized industry default rate
  realized_default_rate <- mean(y_realized)
  
  list(aucs = aucs, default_rate_dist = default_rate_dist, 
       realized_default_rate = realized_default_rate, yh_i = yh_i)
})

#####
# plot of default rate distributions
xs <- local({
  out <- as.character(sapply(forecast_summary, "[[", "yh_i"))
  y <- as.integer(gsub("^(Y)(\\d{4})(.+)", "\\2", out, perl = TRUE))
  H <- as.integer(gsub("^(.+H)(\\d)$", "\\2", out, perl = TRUE))
  y + H / 2 # note that we add an extra half-year as this is period start 
            # of the forecast
})
realized_default_rate <- 
  sapply(forecast_summary, "[[", "realized_default_rate")
qs <- lapply(forecast_summary, "[[", "default_rate_dist")

n_models <- ncol(qs[[1L]])
xs_i <- outer(seq(-.1, .1, length.out = n_models), xs, "+")

pa_func <- function()
  par(mar = c(4, 3, .5, .5), mfcol = c(1, 1), xpd = TRUE, mgp = c(2, 1, 0), 
      cex.lab = .8, cex.axis = .8)
get_plot_device({
  pa_func()
  ylim <- range(unlist(qs), realized_default_rate)
  ylim[2] <- ylim[2] + .04 * diff(ylim)
  plot(
    xs, realized_default_rate, ylim = ylim, type = "n",
    xlab = "(t - 1/2, t]", ylab = "Default rate", xlim = range(xs_i), yaxs= "i", 
    cex = .7, cex.lab = .8,cex.axis = .8)
  
  pch <- c(5, 6, 17, 18)
  for(i in seq_along(qs)){
    xs_i_j <- xs_i[, i]
    qs_j <- qs[[i]]

    # add prediction intervals    
    arrows(xs_i_j, qs_j[1, ], xs_i_j, qs_j[3, ], length = 0.05, angle = 90, 
       code = 0, col = "DarkGray", cex = .7)
    points(xs_i_j, qs_j[1, ], pch = pch, cex = .33)
    points(xs_i_j, qs_j[3, ], pch = pch, cex = .33)
    
    # add medians
    points(xs_i_j, qs_j[2, ], pch = pch, cex = .7)
    
    # add realized 
    points(xs, realized_default_rate, pch = 1, col = rgb(0, 0, 0, alpha = .5), 
           cex = .9)
  }
  
  nber_poly()
}, "default-rate-out-sample-monthly", set_par = FALSE)

# compute number of breaches 
local({
  #####
  # compute figures 
  upper_bounds <- t(sapply(qs, "[", i = 3, j = TRUE))
  breaches <- realized_default_rate >  upper_bounds
  cat("Breaches (>)\n")
  print(n_ge <- colSums(breaches))
  print(colMeans(breaches))
  
  breaches <- realized_default_rate >= upper_bounds
  cat("Breaches (>=)\n")
  print(n_geq <- colSums(breaches))
  print(colMeans(breaches))
  
  lvls <- as.character(sapply(forecast_summary, "[[", "yh_i"))
  lvls <- as.integer(unique(dat$yh[dat$yh %in% lvls]))  + 1L
  lvl_dat <- subset(dat, (as.integer(yh) + 1L) %in% lvls)
  cat("Number of firms and events\n")
  n_obs   <- tapply(
    lvl_dat$mast_issr_num, droplevels(lvl_dat$yh), 
    function(x) length(unique(x)))
  n_events <- tapply(lvl_dat$y, droplevels(lvl_dat$yh), sum)
  
  print(rbind(n_obs = n_obs, n_events = n_events))
  
  #####
  # make table
  nl <- "\\\\\n"
  cat(
    "\\begin{tabular}{lrrrr}\\toprule\n", 
    "&", paste0("\\monthM{", c(1, 3:5), "}", collapse = "& "), nl,
    "\\midrule\n",
    "\\# breaches ($>$) &" , paste0(n_ge , collapse = " & ") , nl,
    "\\# breaches ($\\geq$) &", paste0(n_geq, collapse = " & "), nl,
    "\\# periods &", paste0(rep(length(n_obs), 4), collapse = " & "), nl,
    "\\bottomrule\n",
    "\\end{tabular}",
    sep = "")
})

#####
# AUC
out_auc <- sapply(forecast_summary, "[[","aucs")
xs_i <- outer(seq(-.1, .1, length.out = n_models), xs, "+")

func <- function(y, xs){
  pa_func()
  plot(xs_i, y, type = "n", ylim = range(y, 1, na.rm = TRUE), ylab = "AUC", 
       xlab = "(t - 1/2, t]")
  for(j in 1:ncol(xs_i)){
    # make largest AUC black, lowest with dark blue and the rest dark gray
    xs_i_j <- xs_i[, j]
    y_j <- y[, j]
    col <- ifelse(1:nrow(xs_i) == which.max(y_j), "Black", "DarkGray")
    col[which.min(y_j)] <- "DarkBlue"
    
    points(xs_i_j, y_j, col = col, pch = pch)
  }
  for(i in 1:n_models)
    
  abline(v = head(xs, -1) + .5 * diff(xs), lty = 3, xpd = FALSE)
  nber_poly()
}

get_plot_device(func(out_auc, xs), "auc-out-sample-monthly")

# stats on AUC rankings
table(unlist(apply(out_auc, 2L, which.max)))
```

</div>

## References